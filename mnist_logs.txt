/home/sai/anaconda2/bin/python "/mnt/D/Learning/MTSS/Sem 4/code/designing-neural-networks/src/com/designingnn/Main.py" -dataset mnist
Using TensorFlow backend.

    Starting the application with the following options

    dataset: mnist


generated new net, invalid config of net
Ready to train [C(256,3,1), C(512,3,1), D(1,1), SM(10)]
start, 0, 1, 0, 0, 28, 0, 0
conv, 1, 256, 3, 1, 28, 0, 0
conv, 2, 512, 3, 1, 28, 0, 0
dropout, 2, 1, 0, 0, 28, 1, 0
conv, 3, 512, 3, 1, 28, 0, 1
10
_________________________________________________________________
Layer (type)                 Output Shape              Param #
=================================================================
conv2d_1 (Conv2D)            (None, 28, 28, 256)       7168
_________________________________________________________________
conv2d_2 (Conv2D)            (None, 28, 28, 512)       1180160
_________________________________________________________________
dropout_1 (Dropout)          (None, 28, 28, 512)       0
_________________________________________________________________
flatten_1 (Flatten)          (None, 401408)            0
_________________________________________________________________
dense_1 (Dense)              (None, 10)                4014090
=================================================================
Total params: 5,201,418
Trainable params: 5,201,418
Non-trainable params: 0
_________________________________________________________________
2018-10-13 18:20:31.864889: I tensorflow/core/platform/cpu_feature_guard.cc:141] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX2 FMA
Train on 48000 samples, validate on 12000 samples
2018-10-13 18:20:31.918849: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:897] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2018-10-13 18:20:31.919218: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1392] Found device 0 with properties:
name: GeForce GTX 960M major: 5 minor: 0 memoryClockRate(GHz): 1.176
pciBusID: 0000:01:00.0
totalMemory: 3.95GiB freeMemory: 2.87GiB
2018-10-13 18:20:31.919230: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1471] Adding visible gpu devices: 0
2018-10-13 18:20:32.399900: I tensorflow/core/common_runtime/gpu/gpu_device.cc:952] Device interconnect StreamExecutor with strength 1 edge matrix:
2018-10-13 18:20:32.399925: I tensorflow/core/common_runtime/gpu/gpu_device.cc:958]      0
2018-10-13 18:20:32.399950: I tensorflow/core/common_runtime/gpu/gpu_device.cc:971] 0:   N
2018-10-13 18:20:32.400071: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1084] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 2583 MB memory) -> physical GPU (device: 0, name: GeForce GTX 960M, pci bus id: 0000:01:00.0, compute capability: 5.0)
Epoch 1/20
 - 186s - loss: 0.2835 - acc: 0.9524 - val_loss: 0.1191 - val_acc: 0.9807
Epoch 2/20
 - 183s - loss: 0.0959 - acc: 0.9811 - val_loss: 0.0841 - val_acc: 0.9828
Epoch 3/20
 - 181s - loss: 0.0667 - acc: 0.9861 - val_loss: 0.0719 - val_acc: 0.9846
Epoch 4/20
 - 181s - loss: 0.0536 - acc: 0.9889 - val_loss: 0.0691 - val_acc: 0.9855
Epoch 5/20
 - 182s - loss: 0.0441 - acc: 0.9914 - val_loss: 0.0622 - val_acc: 0.9860
Epoch 6/20
 - 191s - loss: 0.0376 - acc: 0.9930 - val_loss: 0.0645 - val_acc: 0.9869
Epoch 7/20
 - 182s - loss: 0.0333 - acc: 0.9936 - val_loss: 0.0636 - val_acc: 0.9851
Epoch 8/20
 - 179s - loss: 0.0292 - acc: 0.9949 - val_loss: 0.0642 - val_acc: 0.9859
Epoch 9/20
 - 182s - loss: 0.0273 - acc: 0.9956 - val_loss: 0.0621 - val_acc: 0.9863
Epoch 10/20
 - 179s - loss: 0.0249 - acc: 0.9963 - val_loss: 0.0650 - val_acc: 0.9868
Epoch 11/20
 - 179s - loss: 0.0225 - acc: 0.9968 - val_loss: 0.0632 - val_acc: 0.9864
Epoch 12/20
 - 179s - loss: 0.0211 - acc: 0.9974 - val_loss: 0.0656 - val_acc: 0.9863
Epoch 13/20
 - 179s - loss: 0.0205 - acc: 0.9972 - val_loss: 0.0641 - val_acc: 0.9855
Epoch 14/20
 - 179s - loss: 0.0187 - acc: 0.9980 - val_loss: 0.0635 - val_acc: 0.9860
Epoch 15/20
 - 179s - loss: 0.0184 - acc: 0.9980 - val_loss: 0.0648 - val_acc: 0.9873
Epoch 16/20
 - 179s - loss: 0.0174 - acc: 0.9981 - val_loss: 0.0650 - val_acc: 0.9859
Epoch 17/20
 - 180s - loss: 0.0171 - acc: 0.9981 - val_loss: 0.0627 - val_acc: 0.9864
Epoch 18/20
 - 181s - loss: 0.0162 - acc: 0.9984 - val_loss: 0.0665 - val_acc: 0.9857
Epoch 19/20
 - 183s - loss: 0.0156 - acc: 0.9984 - val_loss: 0.0634 - val_acc: 0.9873
Epoch 20/20
 - 179s - loss: 0.0145 - acc: 0.9989 - val_loss: 0.0651 - val_acc: 0.9857
OUT {'status': 'SUCCESS', 'learning_rate': 0.001, 'test_accs': {1: 0.9523958328645676, 2: 0.9811041707793872, 3: 0.9861458373566468, 4: 0.9889375034098824, 5: 0.9914166705061992, 6: 0.9930000041921934, 7: 0.9935625035936634, 8: 0.9949375032757719, 9: 0.9956250031168262, 10: 0.9963125027095278, 11: 0.9968333354840676, 12: 0.9973541688794891, 13: 0.9971666688720385, 14: 0.9979583352183302, 15: 0.9979791684697071, 16: 0.9980625015993912, 17: 0.9980833349128564, 18: 0.9983750014255445, 19: 0.9984166679903865, 20: 0.9988541676973303}}
Ready to train [C(256,5,1), C(64,1,1), D(1,2), C(256,3,1), C(64,5,1), D(2,2), SM(10)]
start, 0, 1, 0, 0, 28, 0, 0
conv, 1, 256, 5, 1, 28, 0, 0
conv, 2, 64, 1, 1, 28, 0, 0
dropout, 2, 1, 0, 0, 28, 2, 0
conv, 3, 256, 3, 1, 28, 0, 0
conv, 4, 64, 5, 1, 28, 0, 0
dropout, 4, 2, 0, 0, 28, 2, 0
conv, 5, 64, 5, 1, 28, 0, 1
10
_________________________________________________________________
Layer (type)                 Output Shape              Param #
=================================================================
conv2d_3 (Conv2D)            (None, 28, 28, 256)       19456
_________________________________________________________________
conv2d_4 (Conv2D)            (None, 28, 28, 64)        16448
_________________________________________________________________
dropout_2 (Dropout)          (None, 28, 28, 64)        0
_________________________________________________________________
conv2d_5 (Conv2D)            (None, 28, 28, 256)       147712
_________________________________________________________________
conv2d_6 (Conv2D)            (None, 28, 28, 64)        409664
_________________________________________________________________
dropout_3 (Dropout)          (None, 28, 28, 64)        0
_________________________________________________________________
flatten_2 (Flatten)          (None, 50176)             0
_________________________________________________________________
dense_2 (Dense)              (None, 10)                501770
=================================================================
Total params: 1,095,050
Trainable params: 1,095,050
Non-trainable params: 0
_________________________________________________________________
Train on 48000 samples, validate on 12000 samples
Epoch 1/20
 - 90s - loss: 0.2937 - acc: 0.9488 - val_loss: 0.1497 - val_acc: 0.9716
Epoch 2/20
 - 89s - loss: 0.1116 - acc: 0.9802 - val_loss: 0.0819 - val_acc: 0.9858
Epoch 3/20
 - 89s - loss: 0.0818 - acc: 0.9853 - val_loss: 0.0737 - val_acc: 0.9865
Epoch 4/20
 - 89s - loss: 0.0687 - acc: 0.9872 - val_loss: 0.0663 - val_acc: 0.9868
Epoch 5/20
 - 89s - loss: 0.0581 - acc: 0.9893 - val_loss: 0.0609 - val_acc: 0.9884
Epoch 6/20
 - 89s - loss: 0.0526 - acc: 0.9902 - val_loss: 0.0571 - val_acc: 0.9888
Epoch 7/20
 - 89s - loss: 0.0483 - acc: 0.9908 - val_loss: 0.0541 - val_acc: 0.9893
Epoch 8/20
 - 89s - loss: 0.0453 - acc: 0.9916 - val_loss: 0.0529 - val_acc: 0.9900
Epoch 9/20
 - 89s - loss: 0.0417 - acc: 0.9929 - val_loss: 0.0487 - val_acc: 0.9914
Epoch 10/20
 - 89s - loss: 0.0384 - acc: 0.9935 - val_loss: 0.0518 - val_acc: 0.9898
Epoch 11/20
 - 89s - loss: 0.0360 - acc: 0.9941 - val_loss: 0.0531 - val_acc: 0.9891
Epoch 12/20
 - 89s - loss: 0.0353 - acc: 0.9944 - val_loss: 0.0497 - val_acc: 0.9898
Epoch 13/20
 - 89s - loss: 0.0333 - acc: 0.9949 - val_loss: 0.0525 - val_acc: 0.9900
Epoch 14/20
 - 89s - loss: 0.0327 - acc: 0.9949 - val_loss: 0.0499 - val_acc: 0.9903
Epoch 15/20
 - 89s - loss: 0.0317 - acc: 0.9948 - val_loss: 0.0474 - val_acc: 0.9908
Epoch 16/20
 - 89s - loss: 0.0295 - acc: 0.9955 - val_loss: 0.0487 - val_acc: 0.9901
Epoch 17/20
 - 89s - loss: 0.0293 - acc: 0.9956 - val_loss: 0.0478 - val_acc: 0.9899
Epoch 18/20
 - 89s - loss: 0.0277 - acc: 0.9964 - val_loss: 0.0476 - val_acc: 0.9900
Epoch 19/20
 - 96s - loss: 0.0275 - acc: 0.9961 - val_loss: 0.0480 - val_acc: 0.9907
Epoch 20/20
 - 90s - loss: 0.0269 - acc: 0.9959 - val_loss: 0.0488 - val_acc: 0.9903
OUT {'status': 'SUCCESS', 'learning_rate': 0.001, 'test_accs': {1: 0.9488333334835867, 2: 0.9801875029380123, 3: 0.9852500036358833, 4: 0.9871666707719365, 5: 0.989270837418735, 6: 0.9902291704590122, 7: 0.9907708375404278, 8: 0.9916250042617321, 9: 0.9928541703770558, 10: 0.9935416699697573, 11: 0.9941250034918387, 12: 0.9943541701883077, 13: 0.9948541701460878, 14: 0.9948750033974647, 15: 0.99477083714058, 16: 0.9954583362986644, 17: 0.9955833368003368, 18: 0.996395835839212, 19: 0.9960625030721227, 20: 0.9959166695674261}}
generated new net, invalid config of net
Ready to train [C(256,5,1), C(128,3,1), D(1,1), GAP(10), SM(10)]
start, 0, 1, 0, 0, 28, 0, 0
conv, 1, 256, 5, 1, 28, 0, 0
conv, 2, 128, 3, 1, 28, 0, 0
dropout, 2, 1, 0, 0, 28, 1, 0
gap, 3, 0, 0, 0, 1, 0, 0
gap, 4, 0, 0, 0, 1, 0, 1
10
_________________________________________________________________
Layer (type)                 Output Shape              Param #
=================================================================
conv2d_7 (Conv2D)            (None, 28, 28, 256)       19456
_________________________________________________________________
conv2d_8 (Conv2D)            (None, 28, 28, 128)       295040
_________________________________________________________________
dropout_4 (Dropout)          (None, 28, 28, 128)       0
_________________________________________________________________
average_pooling2d_1 (Average (None, 27, 27, 128)       0
_________________________________________________________________
flatten_3 (Flatten)          (None, 93312)             0
_________________________________________________________________
dense_3 (Dense)              (None, 10)                933130
=================================================================
Total params: 1,247,626
Trainable params: 1,247,626
Non-trainable params: 0
_________________________________________________________________
Train on 48000 samples, validate on 12000 samples
Epoch 1/20
 - 68s - loss: 0.2054 - acc: 0.9590 - val_loss: 0.1010 - val_acc: 0.9798
Epoch 2/20
 - 67s - loss: 0.0773 - acc: 0.9845 - val_loss: 0.0771 - val_acc: 0.9833
Epoch 3/20
 - 70s - loss: 0.0565 - acc: 0.9879 - val_loss: 0.0654 - val_acc: 0.9856
Epoch 4/20
 - 68s - loss: 0.0461 - acc: 0.9908 - val_loss: 0.0563 - val_acc: 0.9874
Epoch 5/20
 - 67s - loss: 0.0395 - acc: 0.9920 - val_loss: 0.0525 - val_acc: 0.9887
Epoch 6/20
 - 67s - loss: 0.0354 - acc: 0.9930 - val_loss: 0.0556 - val_acc: 0.9869
Epoch 7/20
 - 67s - loss: 0.0318 - acc: 0.9939 - val_loss: 0.0514 - val_acc: 0.9878
Epoch 8/20
 - 67s - loss: 0.0282 - acc: 0.9949 - val_loss: 0.0505 - val_acc: 0.9890
Epoch 9/20
 - 67s - loss: 0.0264 - acc: 0.9954 - val_loss: 0.0528 - val_acc: 0.9884
Epoch 10/20
 - 67s - loss: 0.0247 - acc: 0.9959 - val_loss: 0.0526 - val_acc: 0.9888
Epoch 11/20
 - 67s - loss: 0.0239 - acc: 0.9960 - val_loss: 0.0496 - val_acc: 0.9895
Epoch 12/20
 - 67s - loss: 0.0222 - acc: 0.9967 - val_loss: 0.0512 - val_acc: 0.9896
Epoch 13/20
 - 67s - loss: 0.0214 - acc: 0.9969 - val_loss: 0.0523 - val_acc: 0.9883
Epoch 14/20
 - 67s - loss: 0.0204 - acc: 0.9967 - val_loss: 0.0501 - val_acc: 0.9898
Epoch 15/20
 - 67s - loss: 0.0196 - acc: 0.9974 - val_loss: 0.0533 - val_acc: 0.9877
Epoch 16/20
 - 67s - loss: 0.0194 - acc: 0.9970 - val_loss: 0.0499 - val_acc: 0.9893
Epoch 17/20
 - 67s - loss: 0.0178 - acc: 0.9977 - val_loss: 0.0511 - val_acc: 0.9890
Epoch 18/20
 - 67s - loss: 0.0177 - acc: 0.9977 - val_loss: 0.0492 - val_acc: 0.9896
Epoch 19/20
 - 67s - loss: 0.0174 - acc: 0.9976 - val_loss: 0.0495 - val_acc: 0.9897
Epoch 20/20
 - 67s - loss: 0.0163 - acc: 0.9982 - val_loss: 0.0494 - val_acc: 0.9890
OUT {'status': 'SUCCESS', 'learning_rate': 0.001, 'test_accs': {1: 0.9590416678848366, 2: 0.9845416707297167, 3: 0.9878541706750791, 4: 0.9908333369841178, 5: 0.9920416707793872, 6: 0.9930416703224182, 7: 0.9939375034843881, 8: 0.9948958367109298, 9: 0.9954166698579987, 10: 0.995937502446274, 11: 0.9959583360701799, 12: 0.996729168916742, 13: 0.996895835796992, 14: 0.9967291691030066, 15: 0.9974375021954377, 16: 0.9970416685566306, 17: 0.997708335146308, 18: 0.9976875019570192, 19: 0.9976458351438244, 20: 0.9981875016043583}}
Ready to train [C(512,5,1), C(128,1,1), D(1,2), C(128,1,1), C(512,5,1), D(2,2), SM(10)]
start, 0, 1, 0, 0, 28, 0, 0
conv, 1, 512, 5, 1, 28, 0, 0
conv, 2, 128, 1, 1, 28, 0, 0
dropout, 2, 1, 0, 0, 28, 2, 0
conv, 3, 128, 1, 1, 28, 0, 0
conv, 4, 512, 5, 1, 28, 0, 0
dropout, 4, 2, 0, 0, 28, 2, 0
conv, 5, 512, 5, 1, 28, 0, 1
10
_________________________________________________________________
Layer (type)                 Output Shape              Param #
=================================================================
conv2d_9 (Conv2D)            (None, 28, 28, 512)       38912
_________________________________________________________________
conv2d_10 (Conv2D)           (None, 28, 28, 128)       65664
_________________________________________________________________
dropout_5 (Dropout)          (None, 28, 28, 128)       0
_________________________________________________________________
conv2d_11 (Conv2D)           (None, 28, 28, 128)       16512
_________________________________________________________________
conv2d_12 (Conv2D)           (None, 28, 28, 512)       1638912
_________________________________________________________________
dropout_6 (Dropout)          (None, 28, 28, 512)       0
_________________________________________________________________
flatten_4 (Flatten)          (None, 401408)            0
_________________________________________________________________
dense_4 (Dense)              (None, 10)                4014090
=================================================================
Total params: 5,774,090
Trainable params: 5,774,090
Non-trainable params: 0
_________________________________________________________________
Train on 48000 samples, validate on 12000 samples
Epoch 1/20
 - 231s - loss: 0.3463 - acc: 0.9547 - val_loss: 0.1424 - val_acc: 0.9814
Epoch 2/20
 - 229s - loss: 0.1153 - acc: 0.9823 - val_loss: 0.1016 - val_acc: 0.9833
Epoch 3/20
 - 229s - loss: 0.0830 - acc: 0.9863 - val_loss: 0.0766 - val_acc: 0.9868
Epoch 4/20
 - 229s - loss: 0.0673 - acc: 0.9882 - val_loss: 0.0721 - val_acc: 0.9875
Epoch 5/20
 - 229s - loss: 0.0575 - acc: 0.9904 - val_loss: 0.0696 - val_acc: 0.9867
Epoch 6/20
 - 229s - loss: 0.0505 - acc: 0.9920 - val_loss: 0.0639 - val_acc: 0.9876
Epoch 7/20
 - 229s - loss: 0.0449 - acc: 0.9928 - val_loss: 0.0658 - val_acc: 0.9879
Epoch 8/20
 - 229s - loss: 0.0412 - acc: 0.9940 - val_loss: 0.0587 - val_acc: 0.9882
Epoch 9/20
 - 244s - loss: 0.0381 - acc: 0.9943 - val_loss: 0.0600 - val_acc: 0.9887
Epoch 10/20
 - 237s - loss: 0.0355 - acc: 0.9950 - val_loss: 0.0565 - val_acc: 0.9898
Epoch 11/20
 - 235s - loss: 0.0333 - acc: 0.9956 - val_loss: 0.0572 - val_acc: 0.9895
Epoch 12/20
 - 229s - loss: 0.0313 - acc: 0.9959 - val_loss: 0.0571 - val_acc: 0.9889
Epoch 13/20
 - 229s - loss: 0.0300 - acc: 0.9964 - val_loss: 0.0569 - val_acc: 0.9889
Epoch 14/20
 - 229s - loss: 0.0285 - acc: 0.9968 - val_loss: 0.0551 - val_acc: 0.9897
Epoch 15/20
 - 229s - loss: 0.0271 - acc: 0.9972 - val_loss: 0.0554 - val_acc: 0.9898
Epoch 16/20
 - 229s - loss: 0.0272 - acc: 0.9968 - val_loss: 0.0563 - val_acc: 0.9893
Epoch 17/20
 - 229s - loss: 0.0257 - acc: 0.9974 - val_loss: 0.0561 - val_acc: 0.9896
Epoch 18/20
 - 229s - loss: 0.0250 - acc: 0.9975 - val_loss: 0.0548 - val_acc: 0.9892
Epoch 19/20
 - 229s - loss: 0.0239 - acc: 0.9977 - val_loss: 0.0581 - val_acc: 0.9900
Epoch 20/20
 - 229s - loss: 0.0235 - acc: 0.9980 - val_loss: 0.0570 - val_acc: 0.9898
OUT {'status': 'SUCCESS', 'learning_rate': 0.001, 'test_accs': {1: 0.9546875011719143, 2: 0.9823333370188873, 3: 0.9862916709855198, 4: 0.9882291705037157, 5: 0.9903958379601439, 6: 0.9920416709035635, 7: 0.9927916704987486, 8: 0.9940208363657196, 9: 0.994270836127301, 10: 0.9949583364650607, 11: 0.9956041696170966, 12: 0.9958541696270307, 13: 0.9963750028361876, 14: 0.9968125026052197, 15: 0.9972291690607865, 16: 0.9967500026648243, 17: 0.9973958354443312, 18: 0.9975208355113864, 19: 0.9977291685218612, 20: 0.9979583350941539}}
Ready to train [C(256,5,1), C(128,5,1), D(1,1), GAP(10), SM(10)]
start, 0, 1, 0, 0, 28, 0, 0
conv, 1, 256, 5, 1, 28, 0, 0
conv, 2, 128, 5, 1, 28, 0, 0
dropout, 2, 1, 0, 0, 28, 1, 0
gap, 3, 0, 0, 0, 1, 0, 0
gap, 4, 0, 0, 0, 1, 0, 1
10
_________________________________________________________________
Layer (type)                 Output Shape              Param #
=================================================================
conv2d_13 (Conv2D)           (None, 28, 28, 256)       19456
_________________________________________________________________
conv2d_14 (Conv2D)           (None, 28, 28, 128)       819328
_________________________________________________________________
dropout_7 (Dropout)          (None, 28, 28, 128)       0
_________________________________________________________________
average_pooling2d_2 (Average (None, 27, 27, 128)       0
_________________________________________________________________
flatten_5 (Flatten)          (None, 93312)             0
_________________________________________________________________
dense_5 (Dense)              (None, 10)                933130
=================================================================
Total params: 1,771,914
Trainable params: 1,771,914
Non-trainable params: 0
_________________________________________________________________
Train on 48000 samples, validate on 12000 samples
Epoch 1/20
 - 85s - loss: 0.1962 - acc: 0.9625 - val_loss: 0.0960 - val_acc: 0.9818
Epoch 2/20
 - 84s - loss: 0.0758 - acc: 0.9846 - val_loss: 0.0644 - val_acc: 0.9872
Epoch 3/20
 - 84s - loss: 0.0548 - acc: 0.9891 - val_loss: 0.0639 - val_acc: 0.9850
Epoch 4/20
 - 84s - loss: 0.0442 - acc: 0.9911 - val_loss: 0.0544 - val_acc: 0.9881
Epoch 5/20
 - 84s - loss: 0.0382 - acc: 0.9925 - val_loss: 0.0541 - val_acc: 0.9881
Epoch 6/20
 - 84s - loss: 0.0327 - acc: 0.9937 - val_loss: 0.0480 - val_acc: 0.9894
Epoch 7/20
 - 84s - loss: 0.0299 - acc: 0.9940 - val_loss: 0.0491 - val_acc: 0.9895
Epoch 8/20
 - 84s - loss: 0.0258 - acc: 0.9956 - val_loss: 0.0518 - val_acc: 0.9885
Epoch 9/20
 - 84s - loss: 0.0236 - acc: 0.9963 - val_loss: 0.0473 - val_acc: 0.9897
Epoch 10/20
 - 84s - loss: 0.0230 - acc: 0.9961 - val_loss: 0.0497 - val_acc: 0.9893
Epoch 11/20
 - 84s - loss: 0.0209 - acc: 0.9966 - val_loss: 0.0469 - val_acc: 0.9899
Epoch 12/20
 - 84s - loss: 0.0197 - acc: 0.9970 - val_loss: 0.0477 - val_acc: 0.9894
Epoch 13/20
 - 84s - loss: 0.0188 - acc: 0.9975 - val_loss: 0.0479 - val_acc: 0.9886
Epoch 14/20
 - 84s - loss: 0.0179 - acc: 0.9973 - val_loss: 0.0476 - val_acc: 0.9897
Epoch 15/20
 - 85s - loss: 0.0166 - acc: 0.9979 - val_loss: 0.0481 - val_acc: 0.9897
Epoch 16/20
 - 85s - loss: 0.0162 - acc: 0.9981 - val_loss: 0.0470 - val_acc: 0.9903
Epoch 17/20
 - 85s - loss: 0.0156 - acc: 0.9984 - val_loss: 0.0503 - val_acc: 0.9878
Epoch 18/20
 - 85s - loss: 0.0152 - acc: 0.9985 - val_loss: 0.0457 - val_acc: 0.9894
Epoch 19/20
 - 85s - loss: 0.0145 - acc: 0.9984 - val_loss: 0.0472 - val_acc: 0.9897
Epoch 20/20
 - 85s - loss: 0.0144 - acc: 0.9985 - val_loss: 0.0458 - val_acc: 0.9898
OUT {'status': 'SUCCESS', 'learning_rate': 0.001, 'test_accs': {1: 0.9625416673991519, 2: 0.984604170297583, 3: 0.9891250044728319, 4: 0.9911250045523048, 5: 0.9925208372374376, 6: 0.9937291694805026, 7: 0.993958337046206, 8: 0.9956458361819387, 9: 0.9962708363309503, 10: 0.9961458361397187, 11: 0.9966458360354106, 12: 0.9970208356156945, 13: 0.9975208354492983, 14: 0.9972916688149174, 15: 0.9978750018402934, 16: 0.9981458349153399, 17: 0.9983750014255445, 18: 0.9984583346173167, 19: 0.9983750011771917, 20: 0.9984791679928701}}
Ready to train [C(256,3,1), C(512,1,1), D(1,2), C(256,3,1), C(64,5,1), D(2,2), SM(10)]
start, 0, 1, 0, 0, 28, 0, 0
conv, 1, 256, 3, 1, 28, 0, 0
conv, 2, 512, 1, 1, 28, 0, 0
dropout, 2, 1, 0, 0, 28, 2, 0
conv, 3, 256, 3, 1, 28, 0, 0
conv, 4, 64, 5, 1, 28, 0, 0
dropout, 4, 2, 0, 0, 28, 2, 0
conv, 5, 64, 5, 1, 28, 0, 1
10
_________________________________________________________________
Layer (type)                 Output Shape              Param #
=================================================================
conv2d_15 (Conv2D)           (None, 28, 28, 256)       7168
_________________________________________________________________
conv2d_16 (Conv2D)           (None, 28, 28, 512)       131584
_________________________________________________________________
dropout_8 (Dropout)          (None, 28, 28, 512)       0
_________________________________________________________________
conv2d_17 (Conv2D)           (None, 28, 28, 256)       1179904
_________________________________________________________________
conv2d_18 (Conv2D)           (None, 28, 28, 64)        409664
_________________________________________________________________
dropout_9 (Dropout)          (None, 28, 28, 64)        0
_________________________________________________________________
flatten_6 (Flatten)          (None, 50176)             0
_________________________________________________________________
dense_6 (Dense)              (None, 10)                501770
=================================================================
Total params: 2,230,090
Trainable params: 2,230,090
Non-trainable params: 0
_________________________________________________________________
Train on 48000 samples, validate on 12000 samples
Epoch 1/20
 - 233s - loss: 0.3325 - acc: 0.9469 - val_loss: 0.1213 - val_acc: 0.9806
Epoch 2/20
 - 226s - loss: 0.1058 - acc: 0.9806 - val_loss: 0.0802 - val_acc: 0.9856
Epoch 3/20
 - 226s - loss: 0.0773 - acc: 0.9850 - val_loss: 0.0768 - val_acc: 0.9853
Epoch 4/20
 - 228s - loss: 0.0622 - acc: 0.9882 - val_loss: 0.0675 - val_acc: 0.9874
Epoch 5/20
 - 228s - loss: 0.0540 - acc: 0.9897 - val_loss: 0.0702 - val_acc: 0.9848
Epoch 6/20
 - 228s - loss: 0.0471 - acc: 0.9914 - val_loss: 0.0575 - val_acc: 0.9900
Epoch 7/20
 - 228s - loss: 0.0427 - acc: 0.9921 - val_loss: 0.0581 - val_acc: 0.9883
Epoch 8/20
 - 228s - loss: 0.0396 - acc: 0.9933 - val_loss: 0.0560 - val_acc: 0.9893
Epoch 9/20
 - 228s - loss: 0.0359 - acc: 0.9943 - val_loss: 0.0571 - val_acc: 0.9892
Epoch 10/20
 - 228s - loss: 0.0340 - acc: 0.9946 - val_loss: 0.0558 - val_acc: 0.9903
Epoch 11/20
 - 228s - loss: 0.0325 - acc: 0.9949 - val_loss: 0.0522 - val_acc: 0.9892
Epoch 12/20
 - 228s - loss: 0.0308 - acc: 0.9951 - val_loss: 0.0516 - val_acc: 0.9904
Epoch 13/20
 - 228s - loss: 0.0294 - acc: 0.9954 - val_loss: 0.0551 - val_acc: 0.9884
Epoch 14/20
 - 230s - loss: 0.0277 - acc: 0.9961 - val_loss: 0.0535 - val_acc: 0.9898
Epoch 15/20
 - 229s - loss: 0.0269 - acc: 0.9965 - val_loss: 0.0534 - val_acc: 0.9899
Epoch 16/20
 - 225s - loss: 0.0258 - acc: 0.9969 - val_loss: 0.0525 - val_acc: 0.9896
Epoch 17/20
 - 225s - loss: 0.0247 - acc: 0.9970 - val_loss: 0.0518 - val_acc: 0.9898
Epoch 18/20
 - 225s - loss: 0.0240 - acc: 0.9973 - val_loss: 0.0519 - val_acc: 0.9905
Epoch 19/20
 - 225s - loss: 0.0241 - acc: 0.9972 - val_loss: 0.0520 - val_acc: 0.9898
Epoch 20/20
 - 225s - loss: 0.0235 - acc: 0.9970 - val_loss: 0.0520 - val_acc: 0.9898
OUT {'status': 'SUCCESS', 'learning_rate': 0.001, 'test_accs': {1: 0.9468958333328676, 2: 0.9805833366389076, 3: 0.9850000036259492, 4: 0.9881875049322844, 5: 0.9897083366910616, 6: 0.991395837875704, 7: 0.9921041710302234, 8: 0.993333337083459, 9: 0.9942500032484531, 10: 0.9946250030150016, 11: 0.9949166702106595, 12: 0.9951458365345994, 13: 0.9954375029851993, 14: 0.996125002639989, 15: 0.9964791690930724, 16: 0.9968541691700618, 17: 0.9970416688049833, 18: 0.9973333355039358, 19: 0.9971666689962149, 20: 0.996958335613211}}
Ready to train [C(256,5,1), GAP(10), SM(10)]
start, 0, 1, 0, 0, 28, 0, 0
conv, 1, 256, 5, 1, 28, 0, 0
gap, 2, 0, 0, 0, 1, 0, 0
gap, 3, 0, 0, 0, 1, 0, 1
10
_________________________________________________________________
Layer (type)                 Output Shape              Param #
=================================================================
conv2d_19 (Conv2D)           (None, 28, 28, 256)       19456
_________________________________________________________________
average_pooling2d_3 (Average (None, 27, 27, 256)       0
_________________________________________________________________
flatten_7 (Flatten)          (None, 186624)            0
_________________________________________________________________
dense_7 (Dense)              (None, 10)                1866250
=================================================================
Total params: 1,885,706
Trainable params: 1,885,706
Non-trainable params: 0
_________________________________________________________________
Train on 48000 samples, validate on 12000 samples
Epoch 1/20
 - 40s - loss: 0.1315 - acc: 0.9619 - val_loss: 0.0749 - val_acc: 0.9802
Epoch 2/20
 - 40s - loss: 0.0531 - acc: 0.9863 - val_loss: 0.0681 - val_acc: 0.9818
Epoch 3/20
 - 40s - loss: 0.0369 - acc: 0.9914 - val_loss: 0.0569 - val_acc: 0.9858
Epoch 4/20
 - 40s - loss: 0.0278 - acc: 0.9942 - val_loss: 0.0587 - val_acc: 0.9853
Epoch 5/20
 - 40s - loss: 0.0236 - acc: 0.9952 - val_loss: 0.0537 - val_acc: 0.9878
Epoch 6/20
 - 40s - loss: 0.0200 - acc: 0.9965 - val_loss: 0.0584 - val_acc: 0.9873
Epoch 7/20
 - 40s - loss: 0.0176 - acc: 0.9972 - val_loss: 0.0566 - val_acc: 0.9861
Epoch 8/20
 - 40s - loss: 0.0157 - acc: 0.9980 - val_loss: 0.0568 - val_acc: 0.9878
Epoch 9/20
 - 40s - loss: 0.0139 - acc: 0.9986 - val_loss: 0.0563 - val_acc: 0.9871
Epoch 10/20
 - 40s - loss: 0.0129 - acc: 0.9986 - val_loss: 0.0546 - val_acc: 0.9875
Epoch 11/20
 - 40s - loss: 0.0120 - acc: 0.9988 - val_loss: 0.0542 - val_acc: 0.9868
Epoch 12/20
 - 40s - loss: 0.0112 - acc: 0.9990 - val_loss: 0.0574 - val_acc: 0.9863
Epoch 13/20
 - 40s - loss: 0.0103 - acc: 0.9994 - val_loss: 0.0574 - val_acc: 0.9872
Epoch 14/20
 - 40s - loss: 0.0099 - acc: 0.9994 - val_loss: 0.0580 - val_acc: 0.9873
Epoch 15/20
 - 40s - loss: 0.0095 - acc: 0.9995 - val_loss: 0.0567 - val_acc: 0.9868
Epoch 16/20
 - 40s - loss: 0.0091 - acc: 0.9995 - val_loss: 0.0569 - val_acc: 0.9878
Epoch 17/20
 - 40s - loss: 0.0086 - acc: 0.9996 - val_loss: 0.0590 - val_acc: 0.9865
Epoch 18/20
 - 41s - loss: 0.0082 - acc: 0.9996 - val_loss: 0.0578 - val_acc: 0.9868
Epoch 19/20
 - 40s - loss: 0.0082 - acc: 0.9996 - val_loss: 0.0581 - val_acc: 0.9876
Epoch 20/20
 - 40s - loss: 0.0077 - acc: 0.9997 - val_loss: 0.0591 - val_acc: 0.9879
OUT {'status': 'SUCCESS', 'learning_rate': 0.001, 'test_accs': {1: 0.9618958342354744, 2: 0.986312504298985, 3: 0.9913750033825636, 4: 0.9942083367456992, 5: 0.9951666699722409, 6: 0.9965208361546198, 7: 0.9971875021234154, 8: 0.9979583349699775, 9: 0.9985625013088186, 10: 0.9986458344385027, 11: 0.998770834505558, 12: 0.9990000006432335, 13: 0.9993541672204932, 14: 0.9993750005339582, 15: 0.9994791671012838, 16: 0.999500000414749, 17: 0.9996041670441628, 18: 0.9995833337306976, 19: 0.9996041670441628, 20: 0.9997083336114884}}
Ready to train [C(128,5,1), C(512,5,1), D(1,2), C(64,5,1), C(512,3,1), D(2,2), SM(10)]
start, 0, 1, 0, 0, 28, 0, 0
conv, 1, 128, 5, 1, 28, 0, 0
conv, 2, 512, 5, 1, 28, 0, 0
dropout, 2, 1, 0, 0, 28, 2, 0
conv, 3, 64, 5, 1, 28, 0, 0
conv, 4, 512, 3, 1, 28, 0, 0
dropout, 4, 2, 0, 0, 28, 2, 0
conv, 5, 512, 3, 1, 28, 0, 1
10
_________________________________________________________________
Layer (type)                 Output Shape              Param #
=================================================================
conv2d_20 (Conv2D)           (None, 28, 28, 128)       9728
_________________________________________________________________
conv2d_21 (Conv2D)           (None, 28, 28, 512)       1638912
_________________________________________________________________
dropout_10 (Dropout)         (None, 28, 28, 512)       0
_________________________________________________________________
conv2d_22 (Conv2D)           (None, 28, 28, 64)        819264
_________________________________________________________________
conv2d_23 (Conv2D)           (None, 28, 28, 512)       295424
_________________________________________________________________
dropout_11 (Dropout)         (None, 28, 28, 512)       0
_________________________________________________________________
flatten_8 (Flatten)          (None, 401408)            0
_________________________________________________________________
dense_8 (Dense)              (None, 10)                4014090
=================================================================
Total params: 6,777,418
Trainable params: 6,777,418
Non-trainable params: 0
_________________________________________________________________
Train on 48000 samples, validate on 12000 samples
Epoch 1/20
2018-10-13 23:29:53.300570: W tensorflow/core/common_runtime/bfc_allocator.cc:219] Allocator (GPU_0_bfc) ran out of memory trying to allocate 1.87GiB. The caller indicates that this is not a failure, but may mean that there could be performance gains if more memory were available.
 - 318s - loss: 0.2764 - acc: 0.9545 - val_loss: 0.1073 - val_acc: 0.9835
Epoch 2/20
 - 316s - loss: 0.0940 - acc: 0.9833 - val_loss: 0.0833 - val_acc: 0.9848
Epoch 3/20
 - 316s - loss: 0.0679 - acc: 0.9874 - val_loss: 0.0667 - val_acc: 0.9873
Epoch 4/20
 - 316s - loss: 0.0541 - acc: 0.9898 - val_loss: 0.0594 - val_acc: 0.9880
Epoch 5/20
 - 316s - loss: 0.0453 - acc: 0.9916 - val_loss: 0.0571 - val_acc: 0.9888
Epoch 6/20
 - 316s - loss: 0.0394 - acc: 0.9934 - val_loss: 0.0561 - val_acc: 0.9886
Epoch 7/20
 - 316s - loss: 0.0356 - acc: 0.9940 - val_loss: 0.0554 - val_acc: 0.9884
Epoch 8/20
 - 316s - loss: 0.0327 - acc: 0.9944 - val_loss: 0.0565 - val_acc: 0.9886
Epoch 9/20
 - 316s - loss: 0.0303 - acc: 0.9949 - val_loss: 0.0532 - val_acc: 0.9893
Epoch 10/20
 - 316s - loss: 0.0270 - acc: 0.9961 - val_loss: 0.0575 - val_acc: 0.9880
Epoch 11/20
 - 316s - loss: 0.0265 - acc: 0.9961 - val_loss: 0.0542 - val_acc: 0.9893
Epoch 12/20
 - 316s - loss: 0.0241 - acc: 0.9966 - val_loss: 0.0563 - val_acc: 0.9883
Epoch 13/20
 - 316s - loss: 0.0234 - acc: 0.9967 - val_loss: 0.0510 - val_acc: 0.9899
Epoch 14/20
 - 316s - loss: 0.0218 - acc: 0.9973 - val_loss: 0.0552 - val_acc: 0.9880
Epoch 15/20
 - 316s - loss: 0.0204 - acc: 0.9976 - val_loss: 0.0538 - val_acc: 0.9892
Epoch 16/20
 - 316s - loss: 0.0201 - acc: 0.9974 - val_loss: 0.0560 - val_acc: 0.9883
Epoch 17/20
 - 316s - loss: 0.0192 - acc: 0.9979 - val_loss: 0.0553 - val_acc: 0.9884
Epoch 18/20
 - 316s - loss: 0.0184 - acc: 0.9980 - val_loss: 0.0527 - val_acc: 0.9900
Epoch 19/20
 - 316s - loss: 0.0179 - acc: 0.9981 - val_loss: 0.0536 - val_acc: 0.9889
Epoch 20/20
 - 316s - loss: 0.0179 - acc: 0.9979 - val_loss: 0.0536 - val_acc: 0.9900
OUT {'status': 'SUCCESS', 'learning_rate': 0.001, 'test_accs': {1: 0.9544791672262363, 2: 0.9832916701212525, 3: 0.987437503101925, 4: 0.9897708371281624, 5: 0.9916041705136498, 6: 0.9934166705235838, 7: 0.9939583366115888, 8: 0.9943958365668853, 9: 0.9949166699623068, 10: 0.9961458362638951, 11: 0.9960625026375055, 12: 0.9966458359112342, 13: 0.996729169289271, 14: 0.9973125018800298, 15: 0.9975833351413409, 16: 0.9974375021333496, 17: 0.9978958352158467, 18: 0.9979583350941539, 19: 0.9981041681642334, 20: 0.997937501470248}}
generated new net, invalid config of net
Ready to train [C(512,3,1), C(256,3,1), D(1,2), C(128,3,1), C(256,1,1), D(2,2), SM(10)]
start, 0, 1, 0, 0, 28, 0, 0
conv, 1, 512, 3, 1, 28, 0, 0
conv, 2, 256, 3, 1, 28, 0, 0
dropout, 2, 1, 0, 0, 28, 2, 0
conv, 3, 128, 3, 1, 28, 0, 0
conv, 4, 256, 1, 1, 28, 0, 0
dropout, 4, 2, 0, 0, 28, 2, 0
conv, 5, 256, 1, 1, 28, 0, 1
10
_________________________________________________________________
Layer (type)                 Output Shape              Param #
=================================================================
conv2d_24 (Conv2D)           (None, 28, 28, 512)       14336
_________________________________________________________________
conv2d_25 (Conv2D)           (None, 28, 28, 256)       1179904
_________________________________________________________________
dropout_12 (Dropout)         (None, 28, 28, 256)       0
_________________________________________________________________
conv2d_26 (Conv2D)           (None, 28, 28, 128)       295040
_________________________________________________________________
conv2d_27 (Conv2D)           (None, 28, 28, 256)       33024
_________________________________________________________________
dropout_13 (Dropout)         (None, 28, 28, 256)       0
_________________________________________________________________
flatten_9 (Flatten)          (None, 200704)            0
_________________________________________________________________
dense_9 (Dense)              (None, 10)                2007050
=================================================================
Total params: 3,529,354
Trainable params: 3,529,354
Non-trainable params: 0
_________________________________________________________________
Train on 48000 samples, validate on 12000 samples
Epoch 1/20
 - 216s - loss: 0.3655 - acc: 0.9507 - val_loss: 0.1650 - val_acc: 0.9763
Epoch 2/20
 - 215s - loss: 0.1268 - acc: 0.9802 - val_loss: 0.0988 - val_acc: 0.9840
Epoch 3/20
 - 215s - loss: 0.0874 - acc: 0.9849 - val_loss: 0.0846 - val_acc: 0.9838
Epoch 4/20
 - 215s - loss: 0.0689 - acc: 0.9875 - val_loss: 0.0772 - val_acc: 0.9859
Epoch 5/20
 - 215s - loss: 0.0589 - acc: 0.9895 - val_loss: 0.0715 - val_acc: 0.9859
Epoch 6/20
 - 215s - loss: 0.0505 - acc: 0.9913 - val_loss: 0.0672 - val_acc: 0.9866
Epoch 7/20
 - 215s - loss: 0.0448 - acc: 0.9930 - val_loss: 0.0660 - val_acc: 0.9862
Epoch 8/20
 - 215s - loss: 0.0416 - acc: 0.9932 - val_loss: 0.0656 - val_acc: 0.9862
Epoch 9/20
 - 215s - loss: 0.0389 - acc: 0.9937 - val_loss: 0.0628 - val_acc: 0.9876
Epoch 10/20
 - 215s - loss: 0.0361 - acc: 0.9950 - val_loss: 0.0646 - val_acc: 0.9872
Epoch 11/20
 - 215s - loss: 0.0339 - acc: 0.9953 - val_loss: 0.0635 - val_acc: 0.9868
Epoch 12/20
 - 215s - loss: 0.0322 - acc: 0.9956 - val_loss: 0.0629 - val_acc: 0.9867
Epoch 13/20
 - 215s - loss: 0.0301 - acc: 0.9961 - val_loss: 0.0646 - val_acc: 0.9864
Epoch 14/20
 - 215s - loss: 0.0289 - acc: 0.9965 - val_loss: 0.0655 - val_acc: 0.9864
Epoch 15/20
 - 215s - loss: 0.0288 - acc: 0.9962 - val_loss: 0.0655 - val_acc: 0.9864
Epoch 16/20
 - 216s - loss: 0.0272 - acc: 0.9968 - val_loss: 0.0642 - val_acc: 0.9869
Epoch 17/20
 - 215s - loss: 0.0259 - acc: 0.9972 - val_loss: 0.0637 - val_acc: 0.9876
Epoch 18/20
 - 215s - loss: 0.0256 - acc: 0.9972 - val_loss: 0.0684 - val_acc: 0.9858
Epoch 19/20
 - 215s - loss: 0.0257 - acc: 0.9972 - val_loss: 0.0635 - val_acc: 0.9877
Epoch 20/20
 - 215s - loss: 0.0240 - acc: 0.9978 - val_loss: 0.0630 - val_acc: 0.9873
OUT {'status': 'SUCCESS', 'learning_rate': 0.001, 'test_accs': {1: 0.950687500120451, 2: 0.9801875036830704, 3: 0.9849166704962651, 4: 0.9874791714673241, 5: 0.9894583374882738, 6: 0.9912708377465606, 7: 0.9929583366339405, 8: 0.9931875038271148, 9: 0.9936875040332477, 10: 0.9949583366513253, 11: 0.995250003101925, 12: 0.9956458364302914, 13: 0.996145835891366, 14: 0.9964791689688961, 15: 0.9961875022699436, 16: 0.9967708356678486, 17: 0.9972291688745221, 18: 0.9972291686261694, 19: 0.99718750230968, 20: 0.9977500018974145}}
Ready to train [C(512,3,1), C(256,5,1), D(1,2), C(256,5,1), C(64,5,1), D(2,2), SM(10)]
start, 0, 1, 0, 0, 28, 0, 0
conv, 1, 512, 3, 1, 28, 0, 0
conv, 2, 256, 5, 1, 28, 0, 0
dropout, 2, 1, 0, 0, 28, 2, 0
conv, 3, 256, 5, 1, 28, 0, 0
conv, 4, 64, 5, 1, 28, 0, 0
dropout, 4, 2, 0, 0, 28, 2, 0
conv, 5, 64, 5, 1, 28, 0, 1
10
_________________________________________________________________
Layer (type)                 Output Shape              Param #
=================================================================
conv2d_28 (Conv2D)           (None, 28, 28, 512)       14336
_________________________________________________________________
conv2d_29 (Conv2D)           (None, 28, 28, 256)       3277056
_________________________________________________________________
dropout_14 (Dropout)         (None, 28, 28, 256)       0
_________________________________________________________________
conv2d_30 (Conv2D)           (None, 28, 28, 256)       1638656
_________________________________________________________________
conv2d_31 (Conv2D)           (None, 28, 28, 64)        409664
_________________________________________________________________
dropout_15 (Dropout)         (None, 28, 28, 64)        0
_________________________________________________________________
flatten_10 (Flatten)         (None, 50176)             0
_________________________________________________________________
dense_10 (Dense)             (None, 10)                501770
=================================================================
Total params: 5,841,482
Trainable params: 5,841,482
Non-trainable params: 0
_________________________________________________________________
Train on 48000 samples, validate on 12000 samples
Epoch 1/20
2018-10-14 02:27:52.179482: W tensorflow/core/common_runtime/bfc_allocator.cc:219] Allocator (GPU_0_bfc) ran out of memory trying to allocate 1.87GiB. The caller indicates that this is not a failure, but may mean that there could be performance gains if more memory were available.
 - 341s - loss: 0.3320 - acc: 0.9469 - val_loss: 0.1158 - val_acc: 0.9817
Epoch 2/20
 - 337s - loss: 0.1058 - acc: 0.9805 - val_loss: 0.0805 - val_acc: 0.9855
Epoch 3/20
2018-10-14 02:43:07.635143: W tensorflow/core/common_runtime/bfc_allocator.cc:219] Allocator (GPU_0_bfc) ran out of memory trying to allocate 1.10GiB. The caller indicates that this is not a failure, but may mean that there could be performance gains if more memory were available.
2018-10-14 02:43:07.635208: W tensorflow/stream_executor/cuda/cuda_dnn.cc:3472]
 - 337s - loss: 0.0768 - acc: 0.9855 - val_loss: 0.0751 - val_acc: 0.9848
Epoch 4/20
 - 337s - loss: 0.0624 - acc: 0.9874 - val_loss: 0.0654 - val_acc: 0.9874
Epoch 5/20
 - 337s - loss: 0.0541 - acc: 0.9899 - val_loss: 0.0541 - val_acc: 0.9883
Epoch 6/20
 - 337s - loss: 0.0472 - acc: 0.9910 - val_loss: 0.0554 - val_acc: 0.9890
Epoch 7/20
 - 337s - loss: 0.0430 - acc: 0.9917 - val_loss: 0.0533 - val_acc: 0.9883
Epoch 8/20
 - 337s - loss: 0.0395 - acc: 0.9927 - val_loss: 0.0496 - val_acc: 0.9898
Epoch 9/20
 - 337s - loss: 0.0370 - acc: 0.9929 - val_loss: 0.0501 - val_acc: 0.9898
Epoch 10/20
 - 337s - loss: 0.0333 - acc: 0.9943 - val_loss: 0.0502 - val_acc: 0.9897
Epoch 11/20
 - 337s - loss: 0.0321 - acc: 0.9945 - val_loss: 0.0460 - val_acc: 0.9906
Epoch 12/20
 - 337s - loss: 0.0310 - acc: 0.9946 - val_loss: 0.0441 - val_acc: 0.9908
Epoch 13/20
 - 337s - loss: 0.0303 - acc: 0.9947 - val_loss: 0.0449 - val_acc: 0.9913
Epoch 14/20
 - 337s - loss: 0.0288 - acc: 0.9950 - val_loss: 0.0467 - val_acc: 0.9908
Epoch 15/20
 - 337s - loss: 0.0271 - acc: 0.9955 - val_loss: 0.0438 - val_acc: 0.9909
Epoch 16/20
 - 337s - loss: 0.0267 - acc: 0.9955 - val_loss: 0.0411 - val_acc: 0.9916
Epoch 17/20
2018-10-14 04:02:01.863138: W tensorflow/core/common_runtime/bfc_allocator.cc:219] Allocator (GPU_0_bfc) ran out of memory trying to allocate 1.10GiB. The caller indicates that this is not a failure, but may mean that there could be performance gains if more memory were available.
2018-10-14 04:02:01.863202: W tensorflow/stream_executor/cuda/cuda_dnn.cc:3472]
 - 337s - loss: 0.0261 - acc: 0.9959 - val_loss: 0.0421 - val_acc: 0.9909
Epoch 18/20
 - 337s - loss: 0.0247 - acc: 0.9958 - val_loss: 0.0423 - val_acc: 0.9918
Epoch 19/20
 - 337s - loss: 0.0237 - acc: 0.9959 - val_loss: 0.0416 - val_acc: 0.9913
Epoch 20/20
 - 337s - loss: 0.0234 - acc: 0.9963 - val_loss: 0.0417 - val_acc: 0.9920
OUT {'status': 'SUCCESS', 'learning_rate': 0.001, 'test_accs': {1: 0.9469166666191692, 2: 0.980458336820205, 3: 0.98550000457714, 4: 0.9874375031640132, 5: 0.9899166711916526, 6: 0.9909583376099665, 7: 0.9917083372051517, 8: 0.9927083374311526, 9: 0.9929166701932748, 10: 0.9942500030621886, 11: 0.9945000034446517, 12: 0.9946041698257129, 13: 0.9946875036383669, 14: 0.9950208367779851, 15: 0.9954791696742177, 16: 0.995479169798394, 17: 0.995937502818803, 18: 0.9957500029355287, 19: 0.9959375030050676, 20: 0.9962916693339745}}
Ready to train [C(256,1,1), C(512,5,1), D(1,2), C(256,3,1), C(256,5,1), D(2,2), SM(10)]
start, 0, 1, 0, 0, 28, 0, 0
conv, 1, 256, 1, 1, 28, 0, 0
conv, 2, 512, 5, 1, 28, 0, 0
dropout, 2, 1, 0, 0, 28, 2, 0
conv, 3, 256, 3, 1, 28, 0, 0
conv, 4, 256, 5, 1, 28, 0, 0
dropout, 4, 2, 0, 0, 28, 2, 0
conv, 5, 256, 5, 1, 28, 0, 1
10
_________________________________________________________________
Layer (type)                 Output Shape              Param #
=================================================================
conv2d_32 (Conv2D)           (None, 28, 28, 256)       1024
_________________________________________________________________
conv2d_33 (Conv2D)           (None, 28, 28, 512)       3277312
_________________________________________________________________
dropout_16 (Dropout)         (None, 28, 28, 512)       0
_________________________________________________________________
conv2d_34 (Conv2D)           (None, 28, 28, 256)       1179904
_________________________________________________________________
conv2d_35 (Conv2D)           (None, 28, 28, 256)       1638656
_________________________________________________________________
dropout_17 (Dropout)         (None, 28, 28, 256)       0
_________________________________________________________________
flatten_11 (Flatten)         (None, 200704)            0
_________________________________________________________________
dense_11 (Dense)             (None, 10)                2007050
=================================================================
Total params: 8,103,946
Trainable params: 8,103,946
Non-trainable params: 0
_________________________________________________________________
Train on 48000 samples, validate on 12000 samples
Epoch 1/20
2018-10-14 04:21:18.680188: W tensorflow/core/common_runtime/bfc_allocator.cc:219] Allocator (GPU_0_bfc) ran out of memory trying to allocate 1.14GiB. The caller indicates that this is not a failure, but may mean that there could be performance gains if more memory were available.
2018-10-14 04:21:18.680243: W tensorflow/stream_executor/cuda/cuda_dnn.cc:3472]
2018-10-14 04:23:54.784586: W tensorflow/core/common_runtime/bfc_allocator.cc:219] Allocator (GPU_0_bfc) ran out of memory trying to allocate 1.14GiB. The caller indicates that this is not a failure, but may mean that there could be performance gains if more memory were available.
2018-10-14 04:23:54.784655: W tensorflow/stream_executor/cuda/cuda_dnn.cc:3472]
2018-10-14 04:23:58.073596: W tensorflow/core/common_runtime/bfc_allocator.cc:219] Allocator (GPU_0_bfc) ran out of memory trying to allocate 1.14GiB. The caller indicates that this is not a failure, but may mean that there could be performance gains if more memory were available.
2018-10-14 04:23:58.073682: W tensorflow/stream_executor/cuda/cuda_dnn.cc:3472]
2018-10-14 04:24:14.447953: W tensorflow/core/common_runtime/bfc_allocator.cc:219] Allocator (GPU_0_bfc) ran out of memory trying to allocate 1.14GiB. The caller indicates that this is not a failure, but may mean that there could be performance gains if more memory were available.
2018-10-14 04:24:14.448021: W tensorflow/stream_executor/cuda/cuda_dnn.cc:3472]
2018-10-14 04:24:17.308035: W tensorflow/core/common_runtime/bfc_allocator.cc:219] Allocator (GPU_0_bfc) ran out of memory trying to allocate 1.14GiB. The caller indicates that this is not a failure, but may mean that there could be performance gains if more memory were available.
2018-10-14 04:24:17.308102: W tensorflow/stream_executor/cuda/cuda_dnn.cc:3472]
2018-10-14 04:24:29.309392: W tensorflow/core/common_runtime/bfc_allocator.cc:219] Allocator (GPU_0_bfc) ran out of memory trying to allocate 1.14GiB. The caller indicates that this is not a failure, but may mean that there could be performance gains if more memory were available.
2018-10-14 04:24:29.309459: W tensorflow/stream_executor/cuda/cuda_dnn.cc:3472]
2018-10-14 04:24:43.915518: W tensorflow/stream_executor/cuda/cuda_dnn.cc:3472]
2018-10-14 04:25:29.879242: W tensorflow/stream_executor/cuda/cuda_dnn.cc:3472]
2018-10-14 04:26:41.141495: W tensorflow/stream_executor/cuda/cuda_dnn.cc:3472]
2018-10-14 04:26:47.048506: W tensorflow/stream_executor/cuda/cuda_dnn.cc:3472]
 - 454s - loss: 0.4044 - acc: 0.9512 - val_loss: 0.1391 - val_acc: 0.9820
Epoch 2/20
2018-10-14 04:29:27.913424: W tensorflow/stream_executor/cuda/cuda_dnn.cc:3472]
2018-10-14 04:29:51.237027: W tensorflow/stream_executor/cuda/cuda_dnn.cc:3472]
2018-10-14 04:30:05.859677: W tensorflow/stream_executor/cuda/cuda_dnn.cc:3472]
2018-10-14 04:31:17.533095: W tensorflow/stream_executor/cuda/cuda_dnn.cc:3472]
2018-10-14 04:33:15.827553: W tensorflow/stream_executor/cuda/cuda_dnn.cc:3472]
 - 450s - loss: 0.1130 - acc: 0.9819 - val_loss: 0.0899 - val_acc: 0.9837
Epoch 3/20
2018-10-14 04:36:47.947282: W tensorflow/stream_executor/cuda/cuda_dnn.cc:3472]
2018-10-14 04:37:28.191663: W tensorflow/stream_executor/cuda/cuda_dnn.cc:3472]
2018-10-14 04:37:31.048527: W tensorflow/stream_executor/cuda/cuda_dnn.cc:3472]
2018-10-14 04:39:41.928578: W tensorflow/stream_executor/cuda/cuda_dnn.cc:3472]
2018-10-14 04:39:53.490296: W tensorflow/stream_executor/cuda/cuda_dnn.cc:3472]
2018-10-14 04:40:37.713485: W tensorflow/stream_executor/cuda/cuda_dnn.cc:3472]
2018-10-14 04:41:44.170955: W tensorflow/stream_executor/cuda/cuda_dnn.cc:3472]
2018-10-14 04:42:15.360153: W tensorflow/stream_executor/cuda/cuda_dnn.cc:3472]
 - 450s - loss: 0.0751 - acc: 0.9867 - val_loss: 0.0793 - val_acc: 0.9843
Epoch 4/20
2018-10-14 04:43:55.642918: W tensorflow/stream_executor/cuda/cuda_dnn.cc:3472]
2018-10-14 04:44:45.562761: W tensorflow/stream_executor/cuda/cuda_dnn.cc:3472]
2018-10-14 04:47:21.548632: W tensorflow/stream_executor/cuda/cuda_dnn.cc:3472]
2018-10-14 04:48:15.868092: W tensorflow/stream_executor/cuda/cuda_dnn.cc:3472]
 - 449s - loss: 0.0596 - acc: 0.9887 - val_loss: 0.0657 - val_acc: 0.9867
Epoch 5/20
2018-10-14 04:51:36.552406: W tensorflow/stream_executor/cuda/cuda_dnn.cc:3472]
2018-10-14 04:54:19.647013: W tensorflow/stream_executor/cuda/cuda_dnn.cc:3472]
2018-10-14 04:54:51.292972: W tensorflow/stream_executor/cuda/cuda_dnn.cc:3472]
2018-10-14 04:55:05.479387: W tensorflow/stream_executor/cuda/cuda_dnn.cc:3472]
2018-10-14 04:55:17.047695: W tensorflow/stream_executor/cuda/cuda_dnn.cc:3472]
2018-10-14 04:56:36.483482: W tensorflow/stream_executor/cuda/cuda_dnn.cc:3472]
 - 450s - loss: 0.0500 - acc: 0.9910 - val_loss: 0.0617 - val_acc: 0.9885
Epoch 6/20
2018-10-14 04:58:44.980656: W tensorflow/stream_executor/cuda/cuda_dnn.cc:3472]
2018-10-14 04:59:00.013717: W tensorflow/stream_executor/cuda/cuda_dnn.cc:3472]
2018-10-14 04:59:25.058854: W tensorflow/stream_executor/cuda/cuda_dnn.cc:3472]
2018-10-14 05:01:26.917698: W tensorflow/stream_executor/cuda/cuda_dnn.cc:3472]
2018-10-14 05:02:24.593390: W tensorflow/stream_executor/cuda/cuda_dnn.cc:3472]
2018-10-14 05:03:19.738282: W tensorflow/stream_executor/cuda/cuda_dnn.cc:3472]
2018-10-14 05:04:26.600816: W tensorflow/stream_executor/cuda/cuda_dnn.cc:3472]
2018-10-14 05:04:41.640232: W tensorflow/stream_executor/cuda/cuda_dnn.cc:3472]
 - 450s - loss: 0.0437 - acc: 0.9930 - val_loss: 0.0597 - val_acc: 0.9887
Epoch 7/20
2018-10-14 05:07:11.518976: W tensorflow/stream_executor/cuda/cuda_dnn.cc:3217]
2018-10-14 05:07:11.519204: W tensorflow/stream_executor/cuda/cuda_dnn.cc:3472]
2018-10-14 05:07:34.694989: W tensorflow/stream_executor/cuda/cuda_dnn.cc:3472]
2018-10-14 05:07:54.981794: W tensorflow/stream_executor/cuda/cuda_dnn.cc:3472]
2018-10-14 05:08:17.891569: W tensorflow/stream_executor/cuda/cuda_dnn.cc:3472]
2018-10-14 05:08:30.323336: W tensorflow/stream_executor/cuda/cuda_dnn.cc:3472]
2018-10-14 05:09:07.586035: W tensorflow/stream_executor/cuda/cuda_dnn.cc:3472]
2018-10-14 05:09:16.980986: W tensorflow/stream_executor/cuda/cuda_dnn.cc:3472]
2018-10-14 05:10:42.131872: W tensorflow/stream_executor/cuda/cuda_dnn.cc:3472]
 - 451s - loss: 0.0388 - acc: 0.9939 - val_loss: 0.0584 - val_acc: 0.9879
Epoch 8/20
2018-10-14 05:13:21.542923: W tensorflow/stream_executor/cuda/cuda_dnn.cc:3472]
2018-10-14 05:13:42.682653: W tensorflow/stream_executor/cuda/cuda_dnn.cc:3472]
2018-10-14 05:13:48.173237: W tensorflow/stream_executor/cuda/cuda_dnn.cc:3472]
2018-10-14 05:14:38.056157: W tensorflow/stream_executor/cuda/cuda_dnn.cc:3472]
2018-10-14 05:14:47.863501: W tensorflow/stream_executor/cuda/cuda_dnn.cc:3472]
2018-10-14 05:15:06.760938: W tensorflow/stream_executor/cuda/cuda_dnn.cc:3472]
2018-10-14 05:18:10.265702: W tensorflow/stream_executor/cuda/cuda_dnn.cc:3472]
2018-10-14 05:19:21.065375: W tensorflow/stream_executor/cuda/cuda_dnn.cc:3472]
2018-10-14 05:19:33.495103: W tensorflow/stream_executor/cuda/cuda_dnn.cc:3472]
2018-10-14 05:20:11.626828: W tensorflow/stream_executor/cuda/cuda_dnn.cc:3472]
 - 451s - loss: 0.0351 - acc: 0.9943 - val_loss: 0.0536 - val_acc: 0.9893
Epoch 9/20
2018-10-14 05:20:51.300954: W tensorflow/stream_executor/cuda/cuda_dnn.cc:3472]
2018-10-14 05:21:56.368675: W tensorflow/stream_executor/cuda/cuda_dnn.cc:3472]
2018-10-14 05:22:48.410412: W tensorflow/stream_executor/cuda/cuda_dnn.cc:3472]
2018-10-14 05:24:51.426650: W tensorflow/stream_executor/cuda/cuda_dnn.cc:3472]
2018-10-14 05:25:22.590921: W tensorflow/stream_executor/cuda/cuda_dnn.cc:3472]
2018-10-14 05:26:13.806481: W tensorflow/stream_executor/cuda/cuda_dnn.cc:3472]
2018-10-14 05:27:02.822684: W tensorflow/stream_executor/cuda/cuda_dnn.cc:3472]
 - 450s - loss: 0.0319 - acc: 0.9949 - val_loss: 0.0549 - val_acc: 0.9888
Epoch 10/20
2018-10-14 05:28:32.993067: W tensorflow/stream_executor/cuda/cuda_dnn.cc:3472]
2018-10-14 05:28:52.844566: W tensorflow/stream_executor/cuda/cuda_dnn.cc:3472]
2018-10-14 05:29:43.590043: W tensorflow/stream_executor/cuda/cuda_dnn.cc:3472]
2018-10-14 05:30:07.365394: W tensorflow/stream_executor/cuda/cuda_dnn.cc:3472]
2018-10-14 05:30:31.104724: W tensorflow/stream_executor/cuda/cuda_dnn.cc:3472]
2018-10-14 05:30:32.217362: W tensorflow/stream_executor/cuda/cuda_dnn.cc:3472]
2018-10-14 05:31:06.002630: W tensorflow/stream_executor/cuda/cuda_dnn.cc:3472]
2018-10-14 05:31:48.920438: W tensorflow/stream_executor/cuda/cuda_dnn.cc:3472]
2018-10-14 05:32:24.456868: W tensorflow/stream_executor/cuda/cuda_dnn.cc:3472]
2018-10-14 05:32:32.103868: W tensorflow/stream_executor/cuda/cuda_dnn.cc:3472]
2018-10-14 05:33:15.005658: W tensorflow/stream_executor/cuda/cuda_dnn.cc:3472]
2018-10-14 05:33:24.832920: W tensorflow/stream_executor/cuda/cuda_dnn.cc:3472]
2018-10-14 05:33:28.562621: W tensorflow/stream_executor/cuda/cuda_dnn.cc:3472]
 - 452s - loss: 0.0293 - acc: 0.9957 - val_loss: 0.0507 - val_acc: 0.9903
Epoch 11/20
2018-10-14 05:36:27.194250: W tensorflow/stream_executor/cuda/cuda_dnn.cc:3472]
2018-10-14 05:37:32.314345: W tensorflow/stream_executor/cuda/cuda_dnn.cc:3472]
2018-10-14 05:38:30.064018: W tensorflow/stream_executor/cuda/cuda_dnn.cc:3472]
2018-10-14 05:39:39.524964: W tensorflow/stream_executor/cuda/cuda_dnn.cc:3472]
2018-10-14 05:40:22.873552: W tensorflow/stream_executor/cuda/cuda_dnn.cc:3472]
2018-10-14 05:40:36.615845: W tensorflow/stream_executor/cuda/cuda_dnn.cc:3472]
2018-10-14 05:41:22.578865: W tensorflow/stream_executor/cuda/cuda_dnn.cc:3472]
2018-10-14 05:41:57.656130: W tensorflow/stream_executor/cuda/cuda_dnn.cc:3472]
2018-10-14 05:42:01.388948: W tensorflow/stream_executor/cuda/cuda_dnn.cc:3472]
2018-10-14 05:42:09.916071: W tensorflow/stream_executor/cuda/cuda_dnn.cc:3472]
2018-10-14 05:42:43.248426: W tensorflow/stream_executor/cuda/cuda_dnn.cc:3472]
 - 451s - loss: 0.0280 - acc: 0.9960 - val_loss: 0.0559 - val_acc: 0.9892
Epoch 12/20
2018-10-14 05:44:08.639156: W tensorflow/stream_executor/cuda/cuda_dnn.cc:3472]
2018-10-14 05:44:34.143985: W tensorflow/stream_executor/cuda/cuda_dnn.cc:3472]
2018-10-14 05:46:07.985086: W tensorflow/stream_executor/cuda/cuda_dnn.cc:3472]
2018-10-14 05:46:25.222169: W tensorflow/stream_executor/cuda/cuda_dnn.cc:3472]
2018-10-14 05:47:53.831357: W tensorflow/stream_executor/cuda/cuda_dnn.cc:3472]
2018-10-14 05:48:28.495784: W tensorflow/stream_executor/cuda/cuda_dnn.cc:3472]
2018-10-14 05:49:41.880894: W tensorflow/stream_executor/cuda/cuda_dnn.cc:3472]
 - 450s - loss: 0.0261 - acc: 0.9963 - val_loss: 0.0658 - val_acc: 0.9855
Epoch 13/20
2018-10-14 05:52:15.927057: W tensorflow/stream_executor/cuda/cuda_dnn.cc:3472]
2018-10-14 05:54:35.967471: W tensorflow/stream_executor/cuda/cuda_dnn.cc:3472]
2018-10-14 05:55:42.378214: W tensorflow/stream_executor/cuda/cuda_dnn.cc:3472]
2018-10-14 05:56:03.110724: W tensorflow/stream_executor/cuda/cuda_dnn.cc:3472]
2018-10-14 05:57:06.477017: W tensorflow/stream_executor/cuda/cuda_dnn.cc:3472]
2018-10-14 05:57:33.698980: W tensorflow/stream_executor/cuda/cuda_dnn.cc:3472]
 - 450s - loss: 0.0232 - acc: 0.9973 - val_loss: 0.0579 - val_acc: 0.9878
Epoch 14/20
2018-10-14 05:58:27.320595: W tensorflow/stream_executor/cuda/cuda_dnn.cc:3472]
2018-10-14 05:59:26.347853: W tensorflow/stream_executor/cuda/cuda_dnn.cc:3472]
2018-10-14 06:00:25.803579: W tensorflow/stream_executor/cuda/cuda_dnn.cc:3472]
2018-10-14 06:00:26.486385: W tensorflow/stream_executor/cuda/cuda_dnn.cc:3472]
2018-10-14 06:02:17.160405: W tensorflow/stream_executor/cuda/cuda_dnn.cc:3472]
2018-10-14 06:02:49.300875: W tensorflow/stream_executor/cuda/cuda_dnn.cc:3472]
2018-10-14 06:03:26.840867: W tensorflow/stream_executor/cuda/cuda_dnn.cc:3472]
2018-10-14 06:04:48.941354: W tensorflow/stream_executor/cuda/cuda_dnn.cc:3472]
 - 450s - loss: 0.0232 - acc: 0.9971 - val_loss: 0.0556 - val_acc: 0.9894
Epoch 15/20
2018-10-14 06:06:10.834915: W tensorflow/stream_executor/cuda/cuda_dnn.cc:3472]
2018-10-14 06:07:16.784040: W tensorflow/stream_executor/cuda/cuda_dnn.cc:3472]
2018-10-14 06:07:49.689515: W tensorflow/stream_executor/cuda/cuda_dnn.cc:3472]
2018-10-14 06:08:11.284051: W tensorflow/stream_executor/cuda/cuda_dnn.cc:3472]
2018-10-14 06:08:18.506671: W tensorflow/stream_executor/cuda/cuda_dnn.cc:3472]
2018-10-14 06:08:29.633825: W tensorflow/stream_executor/cuda/cuda_dnn.cc:3472]
2018-10-14 06:09:37.331464: W tensorflow/stream_executor/cuda/cuda_dnn.cc:3472]
2018-10-14 06:09:54.976045: W tensorflow/stream_executor/cuda/cuda_dnn.cc:3472]
2018-10-14 06:11:49.276178: W tensorflow/stream_executor/cuda/cuda_dnn.cc:3472]
2018-10-14 06:12:19.565035: W tensorflow/stream_executor/cuda/cuda_dnn.cc:3472]
2018-10-14 06:12:32.007239: W tensorflow/stream_executor/cuda/cuda_dnn.cc:3472]
2018-10-14 06:12:47.041587: W tensorflow/stream_executor/cuda/cuda_dnn.cc:3472]
 - 451s - loss: 0.0214 - acc: 0.9975 - val_loss: 0.0536 - val_acc: 0.9898
Epoch 16/20
2018-10-14 06:14:19.814986: W tensorflow/stream_executor/cuda/cuda_dnn.cc:3472]
2018-10-14 06:15:31.010138: W tensorflow/stream_executor/cuda/cuda_dnn.cc:3472]
2018-10-14 06:15:46.920672: W tensorflow/stream_executor/cuda/cuda_dnn.cc:3472]
2018-10-14 06:15:59.789126: W tensorflow/stream_executor/cuda/cuda_dnn.cc:3472]
2018-10-14 06:16:23.130682: W tensorflow/stream_executor/cuda/cuda_dnn.cc:3472]
2018-10-14 06:17:01.697681: W tensorflow/stream_executor/cuda/cuda_dnn.cc:3472]
2018-10-14 06:17:36.312396: W tensorflow/stream_executor/cuda/cuda_dnn.cc:3472]
2018-10-14 06:17:41.786183: W tensorflow/stream_executor/cuda/cuda_dnn.cc:3472]
2018-10-14 06:20:03.115640: W tensorflow/stream_executor/cuda/cuda_dnn.cc:3472]
2018-10-14 06:20:12.525307: W tensorflow/stream_executor/cuda/cuda_dnn.cc:3472]
 - 451s - loss: 0.0206 - acc: 0.9976 - val_loss: 0.0548 - val_acc: 0.9895
Epoch 17/20
2018-10-14 06:20:55.624931: W tensorflow/stream_executor/cuda/cuda_dnn.cc:3472]
2018-10-14 06:22:05.484216: W tensorflow/stream_executor/cuda/cuda_dnn.cc:3472]
2018-10-14 06:23:03.648325: W tensorflow/stream_executor/cuda/cuda_dnn.cc:3472]
2018-10-14 06:23:29.090188: W tensorflow/stream_executor/cuda/cuda_dnn.cc:3472]
2018-10-14 06:23:32.383146: W tensorflow/stream_executor/cuda/cuda_dnn.cc:3472]
2018-10-14 06:24:08.766700: W tensorflow/stream_executor/cuda/cuda_dnn.cc:3472]
2018-10-14 06:24:46.040719: W tensorflow/stream_executor/cuda/cuda_dnn.cc:3472]
2018-10-14 06:24:58.908118: W tensorflow/stream_executor/cuda/cuda_dnn.cc:3472]
2018-10-14 06:27:35.849769: W tensorflow/stream_executor/cuda/cuda_dnn.cc:3472]
 - 450s - loss: 0.0196 - acc: 0.9981 - val_loss: 0.0535 - val_acc: 0.9894
Epoch 18/20
2018-10-14 06:28:56.872266: W tensorflow/stream_executor/cuda/cuda_dnn.cc:3472]
2018-10-14 06:32:09.502889: W tensorflow/stream_executor/cuda/cuda_dnn.cc:3472]
2018-10-14 06:32:23.660754: W tensorflow/stream_executor/cuda/cuda_dnn.cc:3472]
2018-10-14 06:32:57.923452: W tensorflow/stream_executor/cuda/cuda_dnn.cc:3472]
2018-10-14 06:33:56.474394: W tensorflow/stream_executor/cuda/cuda_dnn.cc:3472]
2018-10-14 06:34:36.320911: W tensorflow/stream_executor/cuda/cuda_dnn.cc:3472]
 - 450s - loss: 0.0194 - acc: 0.9978 - val_loss: 0.0551 - val_acc: 0.9897
Epoch 19/20
2018-10-14 06:36:06.110266: W tensorflow/stream_executor/cuda/cuda_dnn.cc:3472]
2018-10-14 06:36:18.990526: W tensorflow/stream_executor/cuda/cuda_dnn.cc:3472]
2018-10-14 06:36:52.752968: W tensorflow/stream_executor/cuda/cuda_dnn.cc:3472]
2018-10-14 06:37:46.959414: W tensorflow/stream_executor/cuda/cuda_dnn.cc:3472]
2018-10-14 06:39:02.704538: W tensorflow/stream_executor/cuda/cuda_dnn.cc:3472]
2018-10-14 06:41:26.147394: W tensorflow/stream_executor/cuda/cuda_dnn.cc:3472]
2018-10-14 06:41:38.149786: W tensorflow/stream_executor/cuda/cuda_dnn.cc:3472]
2018-10-14 06:41:48.846687: W tensorflow/stream_executor/cuda/cuda_dnn.cc:3472]
2018-10-14 06:42:18.280353: W tensorflow/stream_executor/cuda/cuda_dnn.cc:3472]
2018-10-14 06:42:39.424703: W tensorflow/stream_executor/cuda/cuda_dnn.cc:3472]
 - 451s - loss: 0.0192 - acc: 0.9979 - val_loss: 0.0525 - val_acc: 0.9893
Epoch 20/20
2018-10-14 06:43:29.007991: W tensorflow/stream_executor/cuda/cuda_dnn.cc:3472]
2018-10-14 06:43:49.701082: W tensorflow/stream_executor/cuda/cuda_dnn.cc:3472]
2018-10-14 06:45:54.045900: W tensorflow/stream_executor/cuda/cuda_dnn.cc:3472]
2018-10-14 06:47:47.356376: W tensorflow/stream_executor/cuda/cuda_dnn.cc:3472]
2018-10-14 06:47:56.321740: W tensorflow/stream_executor/cuda/cuda_dnn.cc:3472]
2018-10-14 06:48:14.891848: W tensorflow/stream_executor/cuda/cuda_dnn.cc:3472]
2018-10-14 06:48:16.014042: W tensorflow/stream_executor/cuda/cuda_dnn.cc:3472]
2018-10-14 06:48:37.631339: W tensorflow/stream_executor/cuda/cuda_dnn.cc:3472]
2018-10-14 06:49:57.511911: W tensorflow/stream_executor/cuda/cuda_dnn.cc:3472]
2018-10-14 06:50:06.249147: W tensorflow/stream_executor/cuda/cuda_dnn.cc:3472]
 - 455s - loss: 0.0187 - acc: 0.9980 - val_loss: 0.0534 - val_acc: 0.9902
OUT {'status': 'SUCCESS', 'learning_rate': 0.001, 'test_accs': {1: 0.9512083346179376, 2: 0.9819166699424386, 3: 0.9866875041897099, 4: 0.9887291713307301, 5: 0.990979170675079, 6: 0.9930000035092235, 7: 0.9938541701063514, 8: 0.9942916702479124, 9: 0.9948750032732884, 10: 0.9957083366811276, 11: 0.9959583360080918, 12: 0.9963333360850811, 13: 0.9973333350693186, 14: 0.9970625021184484, 15: 0.9975208349525928, 16: 0.9976458351438244, 17: 0.9981458349774281, 18: 0.997791668586433, 19: 0.997895834967494, 20: 0.9980208349103729}}
generated new net, invalid config of net
Ready to train [C(256,3,1), C(256,3,1), D(1,2), C(64,1,1), C(512,3,1), D(2,2), SM(10)]
start, 0, 1, 0, 0, 28, 0, 0
conv, 1, 256, 3, 1, 28, 0, 0
conv, 2, 256, 3, 1, 28, 0, 0
dropout, 2, 1, 0, 0, 28, 2, 0
conv, 3, 64, 1, 1, 28, 0, 0
conv, 4, 512, 3, 1, 28, 0, 0
dropout, 4, 2, 0, 0, 28, 2, 0
conv, 5, 512, 3, 1, 28, 0, 1
10
_________________________________________________________________
Layer (type)                 Output Shape              Param #
=================================================================
conv2d_36 (Conv2D)           (None, 28, 28, 256)       7168
_________________________________________________________________
conv2d_37 (Conv2D)           (None, 28, 28, 256)       590080
_________________________________________________________________
dropout_18 (Dropout)         (None, 28, 28, 256)       0
_________________________________________________________________
conv2d_38 (Conv2D)           (None, 28, 28, 64)        16448
_________________________________________________________________
conv2d_39 (Conv2D)           (None, 28, 28, 512)       295424
_________________________________________________________________
dropout_19 (Dropout)         (None, 28, 28, 512)       0
_________________________________________________________________
flatten_12 (Flatten)         (None, 401408)            0
_________________________________________________________________
dense_12 (Dense)             (None, 10)                4014090
=================================================================
Total params: 4,923,210
Trainable params: 4,923,210
Non-trainable params: 0
_________________________________________________________________
Train on 48000 samples, validate on 12000 samples
Epoch 1/20
 - 213s - loss: 0.3041 - acc: 0.9534 - val_loss: 0.1400 - val_acc: 0.9761
Epoch 2/20
 - 212s - loss: 0.1073 - acc: 0.9820 - val_loss: 0.0953 - val_acc: 0.9826
Epoch 3/20
 - 213s - loss: 0.0794 - acc: 0.9856 - val_loss: 0.0757 - val_acc: 0.9859
Epoch 4/20
 - 236s - loss: 0.0645 - acc: 0.9881 - val_loss: 0.0702 - val_acc: 0.9871
Epoch 5/20
 - 217s - loss: 0.0539 - acc: 0.9904 - val_loss: 0.0655 - val_acc: 0.9877
Epoch 6/20
 - 214s - loss: 0.0475 - acc: 0.9921 - val_loss: 0.0645 - val_acc: 0.9876
Epoch 7/20
 - 211s - loss: 0.0432 - acc: 0.9930 - val_loss: 0.0668 - val_acc: 0.9870
Epoch 8/20
 - 213s - loss: 0.0390 - acc: 0.9938 - val_loss: 0.0699 - val_acc: 0.9865
Epoch 9/20
 - 209s - loss: 0.0355 - acc: 0.9947 - val_loss: 0.0675 - val_acc: 0.9863
Epoch 10/20
 - 211s - loss: 0.0339 - acc: 0.9953 - val_loss: 0.0643 - val_acc: 0.9882
Epoch 11/20
 - 216s - loss: 0.0315 - acc: 0.9955 - val_loss: 0.0616 - val_acc: 0.9885
Epoch 12/20
 - 210s - loss: 0.0293 - acc: 0.9963 - val_loss: 0.0632 - val_acc: 0.9875
Epoch 13/20
 - 220s - loss: 0.0287 - acc: 0.9963 - val_loss: 0.0629 - val_acc: 0.9881
Epoch 14/20
 - 212s - loss: 0.0264 - acc: 0.9969 - val_loss: 0.0622 - val_acc: 0.9878
Epoch 15/20
 - 214s - loss: 0.0253 - acc: 0.9972 - val_loss: 0.0620 - val_acc: 0.9883
Epoch 16/20
 - 212s - loss: 0.0243 - acc: 0.9976 - val_loss: 0.0602 - val_acc: 0.9882
Epoch 17/20
 - 215s - loss: 0.0239 - acc: 0.9977 - val_loss: 0.0645 - val_acc: 0.9877
Epoch 18/20
 - 212s - loss: 0.0230 - acc: 0.9977 - val_loss: 0.0644 - val_acc: 0.9877
Epoch 19/20
 - 212s - loss: 0.0222 - acc: 0.9981 - val_loss: 0.0650 - val_acc: 0.9876
Epoch 20/20
 - 215s - loss: 0.0220 - acc: 0.9980 - val_loss: 0.0622 - val_acc: 0.9878
OUT {'status': 'SUCCESS', 'learning_rate': 0.001, 'test_accs': {1: 0.9533750004833564, 2: 0.9819791701311866, 3: 0.9856458373988668, 4: 0.9880625043064356, 5: 0.990416670528551, 6: 0.9920833373442293, 7: 0.9929791700094939, 8: 0.993750003601114, 9: 0.9947291697685917, 10: 0.9952500030398369, 11: 0.9955208361769716, 12: 0.996270835896333, 13: 0.9962916694581508, 14: 0.996916669048369, 15: 0.9972291687503457, 16: 0.9975833350171646, 17: 0.9977083353946606, 18: 0.9976875018328428, 19: 0.9981041681642334, 20: 0.9980416684101026}}
Ready to train [C(128,3,1), GAP(10), SM(10)]
start, 0, 1, 0, 0, 28, 0, 0
conv, 1, 128, 3, 1, 28, 0, 0
gap, 2, 0, 0, 0, 1, 0, 0
gap, 3, 0, 0, 0, 1, 0, 1
10
_________________________________________________________________
Layer (type)                 Output Shape              Param #
=================================================================
conv2d_40 (Conv2D)           (None, 28, 28, 128)       3584
_________________________________________________________________
average_pooling2d_4 (Average (None, 27, 27, 128)       0
_________________________________________________________________
flatten_13 (Flatten)         (None, 93312)             0
_________________________________________________________________
dense_13 (Dense)             (None, 10)                933130
=================================================================
Total params: 936,714
Trainable params: 936,714
Non-trainable params: 0
_________________________________________________________________
Train on 48000 samples, validate on 12000 samples
Epoch 1/20
 - 23s - loss: 0.1625 - acc: 0.9543 - val_loss: 0.0821 - val_acc: 0.9794
Epoch 2/20
 - 23s - loss: 0.0652 - acc: 0.9831 - val_loss: 0.0844 - val_acc: 0.9794
Epoch 3/20
 - 23s - loss: 0.0494 - acc: 0.9876 - val_loss: 0.0707 - val_acc: 0.9823
Epoch 4/20
 - 23s - loss: 0.0409 - acc: 0.9905 - val_loss: 0.0703 - val_acc: 0.9841
Epoch 5/20
 - 23s - loss: 0.0348 - acc: 0.9923 - val_loss: 0.0689 - val_acc: 0.9830
Epoch 6/20
 - 23s - loss: 0.0304 - acc: 0.9939 - val_loss: 0.0722 - val_acc: 0.9827
Epoch 7/20
 - 23s - loss: 0.0269 - acc: 0.9949 - val_loss: 0.0703 - val_acc: 0.9832
Epoch 8/20
 - 23s - loss: 0.0249 - acc: 0.9957 - val_loss: 0.0703 - val_acc: 0.9847
Epoch 9/20
 - 23s - loss: 0.0227 - acc: 0.9962 - val_loss: 0.0720 - val_acc: 0.9820
Epoch 10/20
 - 23s - loss: 0.0214 - acc: 0.9964 - val_loss: 0.0688 - val_acc: 0.9834
Epoch 11/20
 - 23s - loss: 0.0196 - acc: 0.9975 - val_loss: 0.0706 - val_acc: 0.9838
Epoch 12/20
 - 23s - loss: 0.0187 - acc: 0.9975 - val_loss: 0.0757 - val_acc: 0.9824
Epoch 13/20
 - 23s - loss: 0.0177 - acc: 0.9981 - val_loss: 0.0761 - val_acc: 0.9838
Epoch 14/20
 - 23s - loss: 0.0170 - acc: 0.9985 - val_loss: 0.0722 - val_acc: 0.9845
Epoch 15/20
 - 23s - loss: 0.0163 - acc: 0.9984 - val_loss: 0.0747 - val_acc: 0.9834
Epoch 16/20
 - 23s - loss: 0.0156 - acc: 0.9989 - val_loss: 0.0732 - val_acc: 0.9840
Epoch 17/20
 - 23s - loss: 0.0151 - acc: 0.9989 - val_loss: 0.0731 - val_acc: 0.9835
Epoch 18/20
 - 23s - loss: 0.0147 - acc: 0.9990 - val_loss: 0.0739 - val_acc: 0.9838
Epoch 19/20
 - 23s - loss: 0.0141 - acc: 0.9991 - val_loss: 0.0757 - val_acc: 0.9844
Epoch 20/20
 - 23s - loss: 0.0139 - acc: 0.9991 - val_loss: 0.0742 - val_acc: 0.9838
OUT {'status': 'SUCCESS', 'learning_rate': 0.001, 'test_accs': {1: 0.9543125003265838, 2: 0.9831250039860606, 3: 0.9875625041003029, 4: 0.9905000036582351, 5: 0.9922708372895916, 6: 0.9938750035439928, 7: 0.994916670024395, 8: 0.995708336122334, 9: 0.9962291693935792, 10: 0.9963750026499232, 11: 0.9975416683902343, 12: 0.9975416684523225, 13: 0.9980833349128564, 14: 0.9984791680549582, 15: 0.9984166679282983, 16: 0.9988958342621724, 17: 0.9988750010107954, 18: 0.9990208342050513, 19: 0.9991458341479301, 20: 0.9991458341479301}}
Ready to train [C(128,5,1), C(64,3,1), D(1,1), SM(10)]
start, 0, 1, 0, 0, 28, 0, 0
conv, 1, 128, 5, 1, 28, 0, 0
conv, 2, 64, 3, 1, 28, 0, 0
dropout, 2, 1, 0, 0, 28, 1, 0
conv, 3, 64, 3, 1, 28, 0, 1
10
_________________________________________________________________
Layer (type)                 Output Shape              Param #
=================================================================
conv2d_41 (Conv2D)           (None, 28, 28, 128)       9728
_________________________________________________________________
conv2d_42 (Conv2D)           (None, 28, 28, 64)        73792
_________________________________________________________________
dropout_20 (Dropout)         (None, 28, 28, 64)        0
_________________________________________________________________
flatten_14 (Flatten)         (None, 50176)             0
_________________________________________________________________
dense_14 (Dense)             (None, 10)                501770
=================================================================
Total params: 585,290
Trainable params: 585,290
Non-trainable params: 0
_________________________________________________________________
Train on 48000 samples, validate on 12000 samples
Epoch 1/20
 - 32s - loss: 0.2262 - acc: 0.9473 - val_loss: 0.1017 - val_acc: 0.9820
Epoch 2/20
 - 34s - loss: 0.0922 - acc: 0.9814 - val_loss: 0.0742 - val_acc: 0.9852
Epoch 3/20
 - 31s - loss: 0.0672 - acc: 0.9859 - val_loss: 0.0641 - val_acc: 0.9868
Epoch 4/20
 - 31s - loss: 0.0538 - acc: 0.9889 - val_loss: 0.0585 - val_acc: 0.9880
Epoch 5/20
 - 31s - loss: 0.0466 - acc: 0.9902 - val_loss: 0.0585 - val_acc: 0.9866
Epoch 6/20
 - 31s - loss: 0.0423 - acc: 0.9911 - val_loss: 0.0627 - val_acc: 0.9853
Epoch 7/20
 - 31s - loss: 0.0382 - acc: 0.9919 - val_loss: 0.0554 - val_acc: 0.9882
Epoch 8/20
 - 31s - loss: 0.0343 - acc: 0.9931 - val_loss: 0.0518 - val_acc: 0.9887
Epoch 9/20
 - 31s - loss: 0.0318 - acc: 0.9939 - val_loss: 0.0531 - val_acc: 0.9889
Epoch 10/20
 - 31s - loss: 0.0302 - acc: 0.9942 - val_loss: 0.0542 - val_acc: 0.9874
Epoch 11/20
 - 31s - loss: 0.0286 - acc: 0.9945 - val_loss: 0.0548 - val_acc: 0.9870
Epoch 12/20
 - 35s - loss: 0.0261 - acc: 0.9959 - val_loss: 0.0518 - val_acc: 0.9897
Epoch 13/20
 - 31s - loss: 0.0266 - acc: 0.9951 - val_loss: 0.0525 - val_acc: 0.9882
Epoch 14/20
 - 31s - loss: 0.0248 - acc: 0.9960 - val_loss: 0.0512 - val_acc: 0.9894
Epoch 15/20
 - 31s - loss: 0.0238 - acc: 0.9963 - val_loss: 0.0516 - val_acc: 0.9896
Epoch 16/20
 - 31s - loss: 0.0233 - acc: 0.9961 - val_loss: 0.0515 - val_acc: 0.9901
Epoch 17/20
 - 31s - loss: 0.0227 - acc: 0.9965 - val_loss: 0.0522 - val_acc: 0.9883
Epoch 18/20
 - 32s - loss: 0.0216 - acc: 0.9966 - val_loss: 0.0522 - val_acc: 0.9887
Epoch 19/20
 - 31s - loss: 0.0215 - acc: 0.9967 - val_loss: 0.0502 - val_acc: 0.9892
Epoch 20/20
 - 31s - loss: 0.0208 - acc: 0.9968 - val_loss: 0.0495 - val_acc: 0.9898
OUT {'status': 'SUCCESS', 'learning_rate': 0.001, 'test_accs': {1: 0.947333333377416, 2: 0.9813750037923455, 3: 0.9858541702230771, 4: 0.9888958376521866, 5: 0.9902083375801642, 6: 0.9911041702454289, 7: 0.9918750036507845, 8: 0.9931458368276557, 9: 0.9938958372920752, 10: 0.9942291703696052, 11: 0.9945208370685578, 12: 0.9958541696270307, 13: 0.9950833368425568, 14: 0.9960416691377759, 15: 0.9962500026449561, 16: 0.9961458359534542, 17: 0.996500002592802, 18: 0.9966250020389755, 19: 0.9966875026623409, 20: 0.9968333355461557}}
Ready to train [C(256,3,1), C(512,3,1), D(1,1), GAP(10), SM(10)]
start, 0, 1, 0, 0, 28, 0, 0
conv, 1, 256, 3, 1, 28, 0, 0
conv, 2, 512, 3, 1, 28, 0, 0
dropout, 2, 1, 0, 0, 28, 1, 0
gap, 3, 0, 0, 0, 1, 0, 0
gap, 4, 0, 0, 0, 1, 0, 1
10
_________________________________________________________________
Layer (type)                 Output Shape              Param #
=================================================================
conv2d_43 (Conv2D)           (None, 28, 28, 256)       7168
_________________________________________________________________
conv2d_44 (Conv2D)           (None, 28, 28, 512)       1180160
_________________________________________________________________
dropout_21 (Dropout)         (None, 28, 28, 512)       0
_________________________________________________________________
average_pooling2d_5 (Average (None, 27, 27, 512)       0
_________________________________________________________________
flatten_15 (Flatten)         (None, 373248)            0
_________________________________________________________________
dense_15 (Dense)             (None, 10)                3732490
=================================================================
Total params: 4,919,818
Trainable params: 4,919,818
Non-trainable params: 0
_________________________________________________________________
Train on 48000 samples, validate on 12000 samples
Epoch 1/20
 - 212s - loss: 0.2263 - acc: 0.9610 - val_loss: 0.1072 - val_acc: 0.9778
Epoch 2/20
 - 204s - loss: 0.0804 - acc: 0.9832 - val_loss: 0.0744 - val_acc: 0.9837
Epoch 3/20
 - 208s - loss: 0.0577 - acc: 0.9871 - val_loss: 0.0665 - val_acc: 0.9858
Epoch 4/20
 - 206s - loss: 0.0468 - acc: 0.9903 - val_loss: 0.0680 - val_acc: 0.9834
Epoch 5/20
 - 207s - loss: 0.0381 - acc: 0.9922 - val_loss: 0.0615 - val_acc: 0.9867
Epoch 6/20
 - 209s - loss: 0.0338 - acc: 0.9934 - val_loss: 0.0617 - val_acc: 0.9866
Epoch 7/20
 - 204s - loss: 0.0299 - acc: 0.9948 - val_loss: 0.0589 - val_acc: 0.9871
Epoch 8/20
 - 205s - loss: 0.0271 - acc: 0.9952 - val_loss: 0.0610 - val_acc: 0.9868
Epoch 9/20
 - 205s - loss: 0.0243 - acc: 0.9961 - val_loss: 0.0585 - val_acc: 0.9875
Epoch 10/20
 - 205s - loss: 0.0223 - acc: 0.9964 - val_loss: 0.0609 - val_acc: 0.9872
Epoch 11/20
 - 205s - loss: 0.0210 - acc: 0.9969 - val_loss: 0.0583 - val_acc: 0.9871
Epoch 12/20
 - 205s - loss: 0.0196 - acc: 0.9974 - val_loss: 0.0590 - val_acc: 0.9870
Epoch 13/20
 - 205s - loss: 0.0186 - acc: 0.9975 - val_loss: 0.0588 - val_acc: 0.9877
Epoch 14/20
 - 207s - loss: 0.0177 - acc: 0.9977 - val_loss: 0.0591 - val_acc: 0.9867
Epoch 15/20
 - 209s - loss: 0.0171 - acc: 0.9979 - val_loss: 0.0586 - val_acc: 0.9870
Epoch 16/20
 - 211s - loss: 0.0162 - acc: 0.9982 - val_loss: 0.0593 - val_acc: 0.9868
Epoch 17/20
 - 205s - loss: 0.0158 - acc: 0.9984 - val_loss: 0.0617 - val_acc: 0.9868
Epoch 18/20
 - 205s - loss: 0.0150 - acc: 0.9986 - val_loss: 0.0597 - val_acc: 0.9876
Epoch 19/20
 - 205s - loss: 0.0145 - acc: 0.9985 - val_loss: 0.0611 - val_acc: 0.9862
Epoch 20/20
 - 205s - loss: 0.0142 - acc: 0.9988 - val_loss: 0.0600 - val_acc: 0.9863
OUT {'status': 'SUCCESS', 'learning_rate': 0.001, 'test_accs': {1: 0.9610208344956239, 2: 0.9832291703671217, 3: 0.9870833373938998, 4: 0.9903333369642496, 5: 0.9922291706626614, 6: 0.99343750346452, 7: 0.9947500035166741, 8: 0.9952291695401072, 9: 0.9960625027616818, 10: 0.9963958355287711, 11: 0.9969375024239222, 12: 0.9974166688819727, 13: 0.9975416687627633, 14: 0.997708335146308, 15: 0.9978750016540289, 16: 0.9982083346694708, 17: 0.9983958346769214, 18: 0.9986041677494844, 19: 0.9985208344335358, 20: 0.9987916678190232}}
Ready to train [C(256,3,1), C(512,1,1), D(1,2), C(64,5,1), C(256,3,1), D(2,2), SM(10)]
start, 0, 1, 0, 0, 28, 0, 0
conv, 1, 256, 3, 1, 28, 0, 0
conv, 2, 512, 1, 1, 28, 0, 0
dropout, 2, 1, 0, 0, 28, 2, 0
conv, 3, 64, 5, 1, 28, 0, 0
conv, 4, 256, 3, 1, 28, 0, 0
dropout, 4, 2, 0, 0, 28, 2, 0
conv, 5, 256, 3, 1, 28, 0, 1
10
_________________________________________________________________
Layer (type)                 Output Shape              Param #
=================================================================
conv2d_45 (Conv2D)           (None, 28, 28, 256)       7168
_________________________________________________________________
conv2d_46 (Conv2D)           (None, 28, 28, 512)       131584
_________________________________________________________________
dropout_22 (Dropout)         (None, 28, 28, 512)       0
_________________________________________________________________
conv2d_47 (Conv2D)           (None, 28, 28, 64)        819264
_________________________________________________________________
conv2d_48 (Conv2D)           (None, 28, 28, 256)       147712
_________________________________________________________________
dropout_23 (Dropout)         (None, 28, 28, 256)       0
_________________________________________________________________
flatten_16 (Flatten)         (None, 200704)            0
_________________________________________________________________
dense_16 (Dense)             (None, 10)                2007050
=================================================================
Total params: 3,112,778
Trainable params: 3,112,778
Non-trainable params: 0
_________________________________________________________________
Train on 48000 samples, validate on 12000 samples
Epoch 1/20
 - 210s - loss: 0.3067 - acc: 0.9546 - val_loss: 0.1453 - val_acc: 0.9753
Epoch 2/20
 - 207s - loss: 0.1052 - acc: 0.9830 - val_loss: 0.0905 - val_acc: 0.9848
Epoch 3/20
 - 203s - loss: 0.0762 - acc: 0.9868 - val_loss: 0.0794 - val_acc: 0.9850
Epoch 4/20
 - 202s - loss: 0.0601 - acc: 0.9895 - val_loss: 0.0730 - val_acc: 0.9865
Epoch 5/20
 - 203s - loss: 0.0495 - acc: 0.9915 - val_loss: 0.0640 - val_acc: 0.9873
Epoch 6/20
 - 202s - loss: 0.0438 - acc: 0.9929 - val_loss: 0.0619 - val_acc: 0.9880
Epoch 7/20
 - 203s - loss: 0.0400 - acc: 0.9937 - val_loss: 0.0622 - val_acc: 0.9878
Epoch 8/20
 - 203s - loss: 0.0357 - acc: 0.9947 - val_loss: 0.0579 - val_acc: 0.9883
Epoch 9/20
 - 203s - loss: 0.0333 - acc: 0.9950 - val_loss: 0.0583 - val_acc: 0.9880
Epoch 10/20
 - 202s - loss: 0.0314 - acc: 0.9954 - val_loss: 0.0565 - val_acc: 0.9889
Epoch 11/20
 - 202s - loss: 0.0291 - acc: 0.9960 - val_loss: 0.0541 - val_acc: 0.9888
Epoch 12/20
 - 203s - loss: 0.0279 - acc: 0.9962 - val_loss: 0.0542 - val_acc: 0.9898
Epoch 13/20
 - 203s - loss: 0.0258 - acc: 0.9972 - val_loss: 0.0593 - val_acc: 0.9882
Epoch 14/20
 - 206s - loss: 0.0250 - acc: 0.9972 - val_loss: 0.0548 - val_acc: 0.9903
Epoch 15/20
 - 205s - loss: 0.0237 - acc: 0.9975 - val_loss: 0.0558 - val_acc: 0.9897
Epoch 16/20
 - 203s - loss: 0.0230 - acc: 0.9975 - val_loss: 0.0561 - val_acc: 0.9896
Epoch 17/20
 - 203s - loss: 0.0219 - acc: 0.9978 - val_loss: 0.0537 - val_acc: 0.9907
Epoch 18/20
 - 204s - loss: 0.0217 - acc: 0.9979 - val_loss: 0.0560 - val_acc: 0.9898
Epoch 19/20
 - 203s - loss: 0.0207 - acc: 0.9982 - val_loss: 0.0556 - val_acc: 0.9897
Epoch 20/20
 - 209s - loss: 0.0202 - acc: 0.9980 - val_loss: 0.0554 - val_acc: 0.9899
OUT {'status': 'SUCCESS', 'learning_rate': 0.001, 'test_accs': {1: 0.9545625009341165, 2: 0.9829583367332816, 3: 0.9867916706328591, 4: 0.9895000043014686, 5: 0.9915000029529134, 6: 0.9928750041251381, 7: 0.9937291696667672, 8: 0.9947083363309502, 9: 0.9950000029678147, 10: 0.9953541698555152, 11: 0.9960208360105753, 12: 0.996166669142743, 13: 0.9971875021234154, 14: 0.9971875023717681, 15: 0.9974583352605502, 16: 0.9975416687627633, 17: 0.9978125018378099, 18: 0.9978750016540289, 19: 0.9982291681071123, 20: 0.9980208348482847}}
Ready to train [C(64,1,1), C(128,3,1), D(1,2), C(64,3,1), C(256,5,1), D(2,2), SM(10)]
start, 0, 1, 0, 0, 28, 0, 0
conv, 1, 64, 1, 1, 28, 0, 0
conv, 2, 128, 3, 1, 28, 0, 0
dropout, 2, 1, 0, 0, 28, 2, 0
conv, 3, 64, 3, 1, 28, 0, 0
conv, 4, 256, 5, 1, 28, 0, 0
dropout, 4, 2, 0, 0, 28, 2, 0
conv, 5, 256, 5, 1, 28, 0, 1
10
_________________________________________________________________
Layer (type)                 Output Shape              Param #
=================================================================
conv2d_49 (Conv2D)           (None, 28, 28, 64)        256
_________________________________________________________________
conv2d_50 (Conv2D)           (None, 28, 28, 128)       73856
_________________________________________________________________
dropout_24 (Dropout)         (None, 28, 28, 128)       0
_________________________________________________________________
conv2d_51 (Conv2D)           (None, 28, 28, 64)        73792
_________________________________________________________________
conv2d_52 (Conv2D)           (None, 28, 28, 256)       409856
_________________________________________________________________
dropout_25 (Dropout)         (None, 28, 28, 256)       0
_________________________________________________________________
flatten_17 (Flatten)         (None, 200704)            0
_________________________________________________________________
dense_17 (Dense)             (None, 10)                2007050
=================================================================
Total params: 2,564,810
Trainable params: 2,564,810
Non-trainable params: 0
_________________________________________________________________
Train on 48000 samples, validate on 12000 samples
Epoch 1/20
 - 111s - loss: 0.2732 - acc: 0.9540 - val_loss: 0.1201 - val_acc: 0.9844
Epoch 2/20
 - 114s - loss: 0.1009 - acc: 0.9839 - val_loss: 0.0878 - val_acc: 0.9848
Epoch 3/20
 - 108s - loss: 0.0726 - acc: 0.9878 - val_loss: 0.0751 - val_acc: 0.9864
Epoch 4/20
 - 108s - loss: 0.0569 - acc: 0.9907 - val_loss: 0.0641 - val_acc: 0.9879
Epoch 5/20
 - 108s - loss: 0.0471 - acc: 0.9923 - val_loss: 0.0648 - val_acc: 0.9878
Epoch 6/20
 - 110s - loss: 0.0411 - acc: 0.9933 - val_loss: 0.0599 - val_acc: 0.9884
Epoch 7/20
 - 112s - loss: 0.0363 - acc: 0.9941 - val_loss: 0.0612 - val_acc: 0.9873
Epoch 8/20
Traceback (most recent call last):
  File "/mnt/D/Learning/MTSS/Sem 4/code/designing-neural-networks/src/com/designingnn/Main.py", line 27, in <module>
    main()
  File "/mnt/D/Learning/MTSS/Sem 4/code/designing-neural-networks/src/com/designingnn/Main.py", line 23, in main
    DesignNeuralNetwork().start_app()
  File "/mnt/D/Learning/MTSS/Sem 4/code/designing-neural-networks/src/com/designingnn/service/DesignNeuralNetwork.py", line 65, in start_app
    train_out = trainer.run_one_model(net_to_run, iteration)
  File "/mnt/D/Learning/MTSS/Sem 4/code/designing-neural-networks/src/com/designingnn/cnn/ModelRunner.py", line 59, in run_one_model
    callbacks=[tensorboard]
  File "/home/sai/anaconda2/lib/python2.7/site-packages/keras/engine/training.py", line 1037, in fit
    validation_steps=validation_steps)
  File "/home/sai/anaconda2/lib/python2.7/site-packages/keras/engine/training_arrays.py", line 199, in fit_loop
    outs = f(ins_batch)
  File "/home/sai/anaconda2/lib/python2.7/site-packages/keras/backend/tensorflow_backend.py", line 2666, in __call__
    return self._call(inputs)
  File "/home/sai/anaconda2/lib/python2.7/site-packages/keras/backend/tensorflow_backend.py", line 2636, in _call
    fetched = self._callable_fn(*array_vals)
  File "/home/sai/anaconda2/lib/python2.7/site-packages/tensorflow/python/client/session.py", line 1451, in __call__
    self._session._session, self._handle, args, status, None)
KeyboardInterrupt

Process finished with exit code 1
